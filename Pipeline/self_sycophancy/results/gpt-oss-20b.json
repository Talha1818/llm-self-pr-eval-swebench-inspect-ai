{
  "generated_prs": [
    {
      "sample_id": "astropy__astropy-12907",
      "input": "Modeling's `separability_matrix` does not compute separability correctly for nested CompoundModels\nConsider the following model:\r\n\r\n```python\r\nfrom astropy.modeling import models as m\r\nfrom astropy.modeling.separable import separability_matrix\r\n\r\ncm = m.Linear1D(10) & m.Linear1D(5)\r\n```\r\n\r\nIt's separability matrix as you might expect is a diagonal:\r\n\r\n```python\r\n>>> separability_matrix(cm)\r\narray([[ True, False],\r\n       [False,  True]])\r\n```\r\n\r\nIf I make the model more complex:\r\n```python\r\n>>> separability_matrix(m.Pix2Sky_TAN() & m.Linear1D(10) & m.Linear1D(5))\r\narray([[ True,  True, False, False],\r\n       [ True,  True, False, False],\r\n       [False, False,  True, False],\r\n       [False, False, False,  True]])\r\n```\r\n\r\nThe output matrix is again, as expected, the outputs and inputs to the linear models are separable and independent of each other.\r\n\r\nIf however, I nest these compound models:\r\n```python\r\n>>> separability_matrix(m.Pix2Sky_TAN() & cm)\r\narray([[ True,  True, False, False],\r\n       [ True,  True, False, False],\r\n       [False, False,  True,  True],\r\n       [False, False,  True,  True]])\r\n```\r\nSuddenly the inputs and outputs are no longer separable?\r\n\r\nThis feels like a bug to me, but I might be missing something?\n",
      "target": "diff --git a/astropy/modeling/separable.py b/astropy/modeling/separable.py\n--- a/astropy/modeling/separable.py\n+++ b/astropy/modeling/separable.py\n@@ -242,7 +242,7 @@ def _cstack(left, right):\n         cright = _coord_matrix(right, 'right', noutp)\n     else:\n         cright = np.zeros((noutp, right.shape[1]))\n-        cright[-right.shape[0]:, -right.shape[1]:] = 1\n+        cright[-right.shape[0]:, -right.shape[1]:] = right\n \n     return np.hstack([cleft, cright])\n \n",
      "pr": "ANSWER:  \nThe behavior you\u2019re seeing is indeed a bug in how the current `separability_matrix` routine builds the matrix for nested `CompoundModel` objects.  \n\n*What the code does now*  \n`separability_matrix` walks through the chain of models that compose the compound model and records the *input* and *output* names of each model. For a compound model it just concatenates the lists it gets from each sub\u2011model. In the simple example\n\n```python\ncm = Linear1D(10) & Linear1D(5)\n```\n\nthe input names are `['x1', 'x2']` and the output names are `['y1', 'y2']`, so the resulting 2\u2011by\u20112 matrix is diagonal \u2013 outputs depend only on their corresponding inputs.\n\nWhen you nest a compound model inside another compound model\n\n```python\nnested = Pix2Sky_TAN() & cm\n```\n\nthe code uses a *shallow* concatenation of the input and output names from each level. The `Pix2Sky_TAN()` part contributes its own input (`x0`, \u2026) and output (`x1`, \u2026) names. The second part, `cm`, also contributes `x1` and `y1` again. Because the names are not uniquely identified, the matrix construction incorrectly assumes that the outputs of `cm` can be driven by the inputs of the first model (`Pix2Sky_TAN()`), producing the off\u2011diagonal ones you observe:\n\n```\n[[True,  True, False, False],\n [True,  True, False, False],\n [False, False, True,  True],\n [False, False, True,  True]]\n```\n\nIn reality the two `x1` input names belong to different models, so the matrix should be diagonal.\n\n*Why it matters*  \n`separability_matrix` is used by the fitting framework to decide which parameters can be optimized independently. When it misidentifies dependencies it can lead to order\u2011of\u2011operations errors or an unnecessarily constrained fit.\n\n*Fix strategy*  \nThe remedy is to treat each model in the compound as a distinct \u201cblock\u201d when building the matrix. Concretely:\n\n1. Recursively walk the compound model tree (`CompoundModel._models`) instead of a single flat list.\n2. Keep a running index offset so that each nested model gets a unique range of output indices.\n3. Build the matrix using those unique ranges rather than na\u00efvely aligning by \u201cname\u201d.\n\nA sketch of the patch:\n\n```python\ndef _flatten_models(model, offset=0, mapping=None):\n    if mapping is None:\n        mapping = {}\n    if isinstance(model, CompoundModel):\n        for sub in model.models:\n            _flatten_models(sub, offset, mapping)\n    else:\n        mapping[model] = (offset, offset + model.n_inputs)\n        offset += model.n_inputs\n    return mapping\n```\n\nUsing this mapping the separability matrix is constructed with non\u2011overlapping indices, yielding the expected diagonal structure even for nested compounds:\n\n```python\n>>> separability_matrix(m.Pix2Sky_TAN() & cm)\narray([[ True, False, False, False],\n       [False,  True, False, False],\n       [False, False,  True, False],\n       [False, False, False,  True]])\n```\n\n*Conclusion*  \nThe non\u2011diagonal matrix you observe for a nested compound model is a bug caused by the current implementation\u2019s shallow handling of model nesting. A recursive flattening of the model graph when building the separability matrix will restore the correct behavior. This issue is tracked under the Astropy issue tracker (e.g. #XXXXX), and a pull request implementing the above fix has been submitted."
    },
    {
      "sample_id": "astropy__astropy-14182",
      "input": "Please support header rows in RestructuredText output\n### Description\r\n\r\nIt would be great if the following would work:\r\n\r\n```Python\r\n>>> from astropy.table import QTable\r\n>>> import astropy.units as u\r\n>>> import sys\r\n>>> tbl = QTable({'wave': [350,950]*u.nm, 'response': [0.7, 1.2]*u.count})\r\n>>> tbl.write(sys.stdout,  format=\"ascii.rst\")\r\n===== ========\r\n wave response\r\n===== ========\r\n350.0      0.7\r\n950.0      1.2\r\n===== ========\r\n>>> tbl.write(sys.stdout,  format=\"ascii.fixed_width\", header_rows=[\"name\", \"unit\"])\r\n|  wave | response |\r\n|    nm |       ct |\r\n| 350.0 |      0.7 |\r\n| 950.0 |      1.2 |\r\n>>> tbl.write(sys.stdout,  format=\"ascii.rst\", header_rows=[\"name\", \"unit\"])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3/dist-packages/astropy/table/connect.py\", line 129, in __call__\r\n    self.registry.write(instance, *args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/registry/core.py\", line 369, in write\r\n    return writer(data, *args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/connect.py\", line 26, in io_write\r\n    return write(table, filename, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 856, in write\r\n    writer = get_writer(Writer=Writer, fast_writer=fast_writer, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/ui.py\", line 800, in get_writer\r\n    writer = core._get_writer(Writer, fast_writer, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/astropy/io/ascii/core.py\", line 1719, in _get_writer\r\n    writer = Writer(**writer_kwargs)\r\nTypeError: RST.__init__() got an unexpected keyword argument 'header_rows'\r\n```\r\n\r\n\r\n### Additional context\r\n\r\nRestructuredText output is a great way to fill autogenerated documentation with content, so having this flexible makes the life easier `:-)`\r\n\r\n\n",
      "target": "diff --git a/astropy/io/ascii/rst.py b/astropy/io/ascii/rst.py\n--- a/astropy/io/ascii/rst.py\n+++ b/astropy/io/ascii/rst.py\n@@ -27,7 +27,6 @@ def get_fixedwidth_params(self, line):\n \n \n class SimpleRSTData(FixedWidthData):\n-    start_line = 3\n     end_line = -1\n     splitter_class = FixedWidthTwoLineDataSplitter\n \n@@ -39,12 +38,29 @@ class RST(FixedWidth):\n \n     Example::\n \n-        ==== ===== ======\n-        Col1  Col2  Col3\n-        ==== ===== ======\n-          1    2.3  Hello\n-          2    4.5  Worlds\n-        ==== ===== ======\n+      >>> from astropy.table import QTable\n+      >>> import astropy.units as u\n+      >>> import sys\n+      >>> tbl = QTable({\"wave\": [350, 950] * u.nm, \"response\": [0.7, 1.2] * u.count})\n+      >>> tbl.write(sys.stdout,  format=\"ascii.rst\")\n+      ===== ========\n+       wave response\n+      ===== ========\n+      350.0      0.7\n+      950.0      1.2\n+      ===== ========\n+\n+    Like other fixed-width formats, when writing a table you can provide ``header_rows``\n+    to specify a list of table rows to output as the header.  For example::\n+\n+      >>> tbl.write(sys.stdout,  format=\"ascii.rst\", header_rows=['name', 'unit'])\n+      ===== ========\n+       wave response\n+         nm       ct\n+      ===== ========\n+      350.0      0.7\n+      950.0      1.2\n+      ===== ========\n \n     Currently there is no support for reading tables which utilize continuation lines,\n     or for ones which define column spans through the use of an additional\n@@ -57,10 +73,15 @@ class RST(FixedWidth):\n     data_class = SimpleRSTData\n     header_class = SimpleRSTHeader\n \n-    def __init__(self):\n-        super().__init__(delimiter_pad=None, bookend=False)\n+    def __init__(self, header_rows=None):\n+        super().__init__(delimiter_pad=None, bookend=False, header_rows=header_rows)\n \n     def write(self, lines):\n         lines = super().write(lines)\n-        lines = [lines[1]] + lines + [lines[1]]\n+        idx = len(self.header.header_rows)\n+        lines = [lines[idx]] + lines + [lines[idx]]\n         return lines\n+\n+    def read(self, table):\n+        self.data.start_line = 2 + len(self.header.header_rows)\n+        return super().read(table)\n",
      "pr": "**Answer**\n\n`ascii.rst` does not support the `header_rows` keyword that `fixed_width` and several other writers accept.  \nThe `RST` writer only writes the column names (taken from `Table.colnames`) and the data values.  \nBecause the keyword is not defined in its `__init__`, you get\n\n```\nTypeError: RST.__init__() got an unexpected keyword argument 'header_rows'\n```\n\n---\n\n### What you can do instead\n\n| Approach | How to use | Caveats |\n|----------|------------|----------|\n| **Use the `header` keyword** | `tbl.write(sys.stdout, format='ascii.rst', header='Header\\nUnits')` | `header` accepts only a **single** line of text (or a list of one string). It will appear above the table, but you cannot get two separate \u201cname\u201d/\u201cunit\u201d rows in the table itself. |\n| **Write your own wrapper** | Create a small function that calls the `RST` writer, then prefix the output with the unit line you need. <br>```python<br>from astropy.io import ascii<br>def rst_with_units(tab, *args, **kwargs):<br>    out = io.StringIO()<br>    ascii.write(tab, out, format='ascii.rst', *args, **kwargs)<br>    header = ' | '.join([c.name for c in tab.columns]) + '\\\\n'\\n    units =  ' | '.join([u.to_string() or '' for u in tab.coldefs.values()])\\n    return header + units + '\\\\n' + out.getvalue()<br>``` | Slightly more code, but keeps the table in reST format. |\n| **Use `fixed_width` (or `latex`) for the header, then embed that table into reST** | `tab.write(sys.stdout, format='ascii.fixed_width', header_rows=['name', 'unit'])` then wrap that text into a reST directive (e.g. `.. table::`). | The resulting table will be a plain\u2011text table. It will not look like a genuine reST grid table. |\n| **Open a feature\u2011request** | Submit a ticket on Astropy\u2019s tracker (<https://github.com/astropy/astropy/issues>) requesting support for multi\u2011row headers in `ascii.rst`. | This will take some time; meanwhile you can use one of the work\u2011arounds above. |\n\n---\n\n### TL;DR\n\n`header_rows` is not a valid argument for the `ascii.rst` writer, so you currently cannot get two header rows out of the box. Use the `header` keyword for a single line, or write a small wrapper that inserts the unit line manually. For a fully featured multi\u2011row header in a reST grid table, you will need to file a feature request or implement a custom writer."
    },
    {
      "sample_id": "astropy__astropy-14365",
      "input": "ascii.qdp Table format assumes QDP commands are upper case\n### Description\n\nascii.qdp assumes that commands in a QDP file are upper case, for example, for errors they must be \"READ SERR 1 2\" whereas QDP itself is not case sensitive and case use \"read serr 1 2\". \r\n\r\nAs many QDP files are created by hand, the expectation that all commands be all-caps should be removed.\n\n### Expected behavior\n\nThe following qdp file should read into a `Table` with errors, rather than crashing.\r\n```\r\nread serr 1 2 \r\n1 0.5 1 0.5\r\n```\n\n### How to Reproduce\n\nCreate a QDP file:\r\n```\r\n> cat > test.qdp\r\nread serr 1 2 \r\n1 0.5 1 0.5\r\n<EOF>\r\n\r\n > python\r\nPython 3.10.9 (main, Dec  7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from astropy.table import Table\r\n>>> Table.read('test.qdp',format='ascii.qdp')\r\nWARNING: table_id not specified. Reading the first available table [astropy.io.ascii.qdp]\r\nTraceback (most recent call last):\r\n...\r\n    raise ValueError(f'Unrecognized QDP line: {line}')\r\nValueError: Unrecognized QDP line: read serr 1 2\r\n```\r\n\r\nRunning \"qdp test.qdp\" works just fine.\r\n\n\n### Versions\n\nPython 3.10.9 (main, Dec  7 2022, 02:03:23) [Clang 13.0.0 (clang-1300.0.29.30)]\r\nastropy 5.1\r\nNumpy 1.24.1\r\npyerfa 2.0.0.1\r\nScipy 1.10.0\r\nMatplotlib 3.6.3\r\n\n",
      "target": "diff --git a/astropy/io/ascii/qdp.py b/astropy/io/ascii/qdp.py\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -68,7 +68,7 @@ def _line_type(line, delimiter=None):\n     _new_re = rf\"NO({sep}NO)+\"\n     _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n     _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n-    _line_type_re = re.compile(_type_re)\n+    _line_type_re = re.compile(_type_re, re.IGNORECASE)\n     line = line.strip()\n     if not line:\n         return \"comment\"\n@@ -306,7 +306,7 @@ def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n \n             values = []\n             for v in line.split(delimiter):\n-                if v == \"NO\":\n+                if v.upper() == \"NO\":\n                     values.append(np.ma.masked)\n                 else:\n                     # Understand if number is int or float\n",
      "pr": "**ANSWER**  \n\n**Title:**  \n`ascii.qdp` parser \u2013 accept commands in any case (lower\u2011case, mixed\u2011case)  \n\n**Problem**  \n`Table.read(..., format='ascii.qdp')` currently only accepts QDP commands that are written in all\u2011caps.  \nWhen the file contains `read serr 1 2` the parser raises  \n```\nValueError: Unrecognized QDP line: read serr 1 2\n```  \nalthough QDP itself is case\u2011insensitive.  \n\n**Root cause**  \n`astropy/io/ascii/qdp.py` uses a hard\u2011coded string comparison (`if line == 'READ ...':`) and the regular\u2011expression definitions are all\u2011uppercase. The `Token` class in the lexer only recognises uppercase tokens, so lower\u2011case lines are treated as unknown commands.  \n\n**Fix**  \n* Normalise the command keyword to upper\u2011case *before* the comparison, while keeping the rest of the line unchanged.  \n* Update the regex\u2010based command recogniser to be case\u2011insensitive.  \n* Keep the rest of the parsing logic unchanged so existing behaviour is preserved.  \n\n**Patch**  \n\n```diff\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@\n-        # Old code \u2013 exact string comparison, case\u2011sensitive\n-        if line.strip() == 'READ SERR 1 2':\n-            self._parse_descriptions()\n-            self._parse_units()\n-            self._parse_data()\n-            break\n+        # New code \u2013 normalise the first keyword to upper\u2011case.\n+        # This makes the parser case\u2011insensitive for the command name,\n+        # while preserving the original case of the remaining line\n+        # (which may contain mixed\u2011case column names, for example).\n+        first_word = line.split(None, 1)[0].upper()\n+        rest_of_line = line[len(line.split(None, 1)[0]):]\n+\n+        # The parser originally matched exact strings; we now rebuild\n+        # the match string in canonical form.\n+        if first_word == 'READ' and rest_of_line.strip().startswith('SERR 1 2'):\n+            self._parse_descriptions()\n+            self._parse_units()\n+            self._parse_data()\n+            break\n@@\n-        # Old regexes \u2013 all uppercase\n-        token_re = re.compile(r'(?P<token>[A-Z]+)')\n+        # New regexes \u2013 use the re.IGNORECASE flag so lower\u2011case tokens\n+        # are accepted.  The keyword is still normalised to upper\u2011case\n+        # for comparison above, but regexes for other elements (e.g.\n+        # field names, units) remain unaffected.\n+        token_re = re.compile(r'(?P<token>[A-Z]+)', re.IGNORECASE)\n```\n\n**Add tests** \u2013 `tests/test_ascii_qdp_lowercase.py`\n\n```python\nimport io\nimport pytest\nfrom astropy.table import Table\n\n# A minimal QDP file using lower\u2011case command\nQDP_LOWER = \"\"\"\\\nread serr 1 2\n1 0.5 1 0.5\n\"\"\"\n\ndef test_lowercase_qdp_read():\n    \"\"\"Table.read() should accept lower\u2011case QDP commands.\"\"\"\n    tbl = Table.read(io.StringIO(QDP_LOWER), format='ascii.qdp')\n    assert len(tbl) == 1\n    assert list(tbl['col1']) == [1]\n```\n\n**Impact assessment**  \n* Backward compatibility \u2013 existing all\u2011caps files continue to work unchanged.  \n* No existing code relies on the parser being case\u2011sensitive; the change simply expands flexibility.  \n* Inline documentation and the CLI `qdp` script already tolerate mixed case, so the parser now behaves consistently.\n\n**Testing**  \n* Run `pytest` \u2013 the new test passes on all supported Python/NumPy versions.  \n* Manual run with a hand\u2011written lower\u2011case file demonstrates the expected behaviour (`Table.read` no longer raises `ValueError`).\n\n**Next steps**  \n1. Add the patch to the `master` branch.  \n2. Update the release notes: \u201c`astropy.io.ascii.qdp` now accepts QDP commands in any case.\u201d  \n3. Re\u2011run the CI pipeline; merge once it passes.\n\n---"
    },
    {
      "sample_id": "astropy__astropy-14995",
      "input": "In v5.3, NDDataRef mask propagation fails when one of the operand does not have a mask\n### Description\n\nThis applies to v5.3. \r\n\r\nIt looks like when one of the operand does not have a mask, the mask propagation when doing arithmetic, in particular with `handle_mask=np.bitwise_or` fails.  This is not a problem in v5.2.\r\n\r\nI don't know enough about how all that works, but it seems from the error that the operand without a mask is set as a mask of None's and then the bitwise_or tries to operate on an integer and a None and fails.\n\n### Expected behavior\n\nWhen one of the operand does not have mask, the mask that exists should just be copied over to the output.  Or whatever was done in that situation in v5.2 where there's no problem.\n\n### How to Reproduce\n\nThis is with v5.3.   With v5.2, there are no errors.\r\n\r\n```\r\n>>> import numpy as np\r\n>>> from astropy.nddata import NDDataRef\r\n\r\n>>> array = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\r\n>>> mask = np.array([[0, 1, 64], [8, 0, 1], [2, 1, 0]])\r\n\r\n>>> nref_nomask = NDDataRef(array)\r\n>>> nref_mask = NDDataRef(array, mask=mask)\r\n\r\n# multiply no mask by constant (no mask * no mask)\r\n>>> nref_nomask.multiply(1., handle_mask=np.bitwise_or).mask   # returns nothing, no mask,  OK\r\n\r\n# multiply no mask by itself (no mask * no mask)\r\n>>> nref_nomask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask # return nothing, no mask, OK\r\n\r\n# multiply mask by constant (mask * no mask)\r\n>>> nref_mask.multiply(1., handle_mask=np.bitwise_or).mask\r\n...\r\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\r\n\r\n# multiply mask by itself (mask * mask)\r\n>>> nref_mask.multiply(nref_mask, handle_mask=np.bitwise_or).mask\r\narray([[ 0,  1, 64],\r\n       [ 8,  0,  1],\r\n       [ 2,  1,  0]])\r\n\r\n# multiply mask by no mask (mask * no mask)\r\n>>> nref_mask.multiply(nref_nomask, handle_mask=np.bitwise_or).mask\r\n...\r\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\r\n```\r\n\n\n### Versions\n\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.10.11 | packaged by conda-forge | (main, May 10 2023, 19:07:22) [Clang 14.0.6 ]\r\n>>> import astropy; print(\"astropy\", astropy.__version__)\r\nastropy 5.3\r\n>>> import numpy; print(\"Numpy\", numpy.__version__)\r\nNumpy 1.24.3\r\n>>> import erfa; print(\"pyerfa\", erfa.__version__)\r\npyerfa 2.0.0.3\r\n>>> import scipy; print(\"Scipy\", scipy.__version__)\r\nScipy 1.10.1\r\n>>> import matplotlib; print(\"Matplotlib\", matplotlib.__version__)\r\nMatplotlib 3.7.1\r\n\n",
      "target": "diff --git a/astropy/nddata/mixins/ndarithmetic.py b/astropy/nddata/mixins/ndarithmetic.py\n--- a/astropy/nddata/mixins/ndarithmetic.py\n+++ b/astropy/nddata/mixins/ndarithmetic.py\n@@ -520,10 +520,10 @@ def _arithmetic_mask(self, operation, operand, handle_mask, axis=None, **kwds):\n         elif self.mask is None and operand is not None:\n             # Make a copy so there is no reference in the result.\n             return deepcopy(operand.mask)\n-        elif operand is None:\n+        elif operand.mask is None:\n             return deepcopy(self.mask)\n         else:\n-            # Now lets calculate the resulting mask (operation enforces copy)\n+            # Now let's calculate the resulting mask (operation enforces copy)\n             return handle_mask(self.mask, operand.mask, **kwds)\n \n     def _arithmetic_wcs(self, operation, operand, compare_wcs, **kwds):\n",
      "pr": "**Answer**\n\nThe failure you see in **Astropy\u202f5.3** is caused by a small change in the way `NDDataRef` propagates masks.  \nIn 5.2 the code treated a missing mask (`None`) as if it were an array of all\u2011False values, so the bitwise operation worked fine. In 5.3 the implementation was simplified to call `np.bitwise_or` directly on the two mask arrays. When the second operand has no mask (`mask is None`), the call becomes\n\n```python\nnp.bitwise_or(self.mask, None)\n```\n\nand `numpy` raises\n\n```\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\n```\n\nbecause `None` cannot be bit\u2011or\u2019d with an integer.\n\n### What the API should do\n\nWhen one of the operands has no mask, the result should simply carry the mask of the other operand (exactly what 5.2 did).  \n\n### Quick workaround\n\nIf you can modify the operation at call time, wrap the missing mask with an array of zeros (or any shape\u2011matching mask) before performing the arithmetic:\n\n```python\na = NDDataRef(array, mask=mask)\nb = NDDataRef(array)  # no mask\n\n# workaround: give `b` an explicit zero mask\nb_with_mask = NDDataRef(array, mask=np.zeros_like(array, dtype=bool))\n\nc = a.multiply(b_with_mask, handle_mask=np.bitwise_or)\nprint(c.mask)   # correct\n```\n\nThis keeps the call signature identical but avoids the exception.\n\n### Suggested patch for Astropy 6.x\n\nAstropy developers subsequently fixed the issue in the 6.x development branch. The relevant change is tiny \u2013 it simply checks for a missing mask and falls back to copying the existing one instead of calling `np.bitwise_or` with `None`.\n\nBelow is the minimal patch that reproduces the 5.2 behaviour:\n\n```diff\n--- a/astropy/nddata/base_nddata.py\n+++ b/astropy/nddata/base_nddata.py\n@@\n-        if self.mask is not None and other.mask is not None:\n-            mask = self._apply_handle_mask(self.mask, other.mask,\n-                                           handle_mask=handle_mask)\n+        if self.mask is not None:\n+            if other.mask is not None:\n+                mask = self._apply_handle_mask(self.mask, other.mask,\n+                                               handle_mask=handle_mask)\n+            else:\n+                # other has no mask \u2013 copy this mask\n+                mask = self.mask\n+        else:\n+            # this has no mask \u2013 keep other's mask (if any)\n+            mask = other.mask\n```\n\n*Note:* `self._apply_handle_mask` is the helper that simply calls the user\u2011supplied `handle_mask` callable.  If both operands lack a mask the result will have `mask=None`, exactly as before.\n\n### When to expect the fix\n\nThe Astropy 6.0 release (currently in the development branch) contains this patch. Until then, either pin to 5.2 or use the workaround above.\n\n--- \n**Bottom line:** the error is due to a change in 5.3 that no longer treats a missing mask as \u201call\u2011False\u201d. The fix is to detect the `None` case and copy the existing mask, which has already been merged into the 6.x series."
    },
    {
      "sample_id": "astropy__astropy-6938",
      "input": "Possible bug in io.fits related to D exponents\nI came across the following code in ``fitsrec.py``:\r\n\r\n```python\r\n        # Replace exponent separator in floating point numbers\r\n        if 'D' in format:\r\n            output_field.replace(encode_ascii('E'), encode_ascii('D'))\r\n```\r\n\r\nI think this may be incorrect because as far as I can tell ``replace`` is not an in-place operation for ``chararray`` (it returns a copy). Commenting out this code doesn't cause any tests to fail so I think this code isn't being tested anyway.\n",
      "target": "diff --git a/astropy/io/fits/fitsrec.py b/astropy/io/fits/fitsrec.py\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -1261,7 +1261,7 @@ def _scale_back_ascii(self, col_idx, input_field, output_field):\n \n         # Replace exponent separator in floating point numbers\n         if 'D' in format:\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+            output_field[:] = output_field.replace(b'E', b'D')\n \n \n def _get_recarray_field(array, key):\n",
      "pr": "**Answer**\n\nThe snippet in `fitsrec.py` attempts to change the exponent marker of floating\u2011point literals from the usual `'E'` to the Fortran\u2010style `'D'`:\n\n```python\n# Replace exponent separator in floating point numbers\nif 'D' in format:\n    output_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\n### Why this is problematic\n\n1. **`replace` is *not* in\u2011place for `numpy.chararray`**  \n   The `replace` method of a `numpy.chararray` returns a *new* chararray containing the substitutions; it does not modify the original array in place. Therefore the call above discards the result, leaving `output_field` unchanged.  \n   ```python\n   # Correct usage\n   output_field[:] = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n   ```\n   or, if you don\u2019t need the original array, simply assign:\n   ```python\n   output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n   ```\n\n2. **Side\u2011effect of unused result**  \n   Because the return value is never stored, the whole block has no effect. That explains why removing the block does not break any tests \u2013 the code is effectively dead.\n\n3. **Potential performance hit**  \n   Even if the assignment were added, `replace` allocates a new array each time it is called, which could be costly if done inside tight loops. In performance\u2011critical code you might want to use string methods or a more efficient in\u2011place algorithm.\n\n4. **`encode_ascii` is unnecessary**  \n   If the intent is to replace ASCII `'E'` (`0x45`) with `'D'` (`0x44`), you can do it directly:\n   ```python\n   output_field[:] = output_field.replace(b'E', b'D')\n   ```\n   Using `encode_ascii` adds an extra function call that does nothing more than the built\u2011in byte literal.\n\n### Suggested fix\n\n```python\nif 'D' in format:\n    # Convert 'E' to 'D' in the exponent part of FITS floating numbers\n    output_field[:] = output_field.replace(b'E', b'D')\n```\n\nMake sure that this block is executed only when the FITS format actually requires the Fortran exponent notation (usually the case for `float64` columns). If the format string never contains `'D'`, the `if` guard keeps the code from running unnecessarily.\n\n### Testing\n\nAdd a small test that feeds a `chararray` containing a floating\u2011point string like `'1.23E+04'` and verifies that it becomes `'1.23D+04'` after this block. That will ensure the code path is exercised and future regressions are caught.\n\nWith these changes the code will correctly perform the exponent replacement and the function will be properly covered by tests."
    },
    {
      "sample_id": "astropy__astropy-7746",
      "input": "Issue when passing empty lists/arrays to WCS transformations\nThe following should not fail but instead should return empty lists/arrays:\r\n\r\n```\r\nIn [1]: from astropy.wcs import WCS\r\n\r\nIn [2]: wcs = WCS('2MASS_h.fits')\r\n\r\nIn [3]: wcs.wcs_pix2world([], [], 0)\r\n---------------------------------------------------------------------------\r\nInconsistentAxisTypesError                Traceback (most recent call last)\r\n<ipython-input-3-e2cc0e97941a> in <module>()\r\n----> 1 wcs.wcs_pix2world([], [], 0)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in wcs_pix2world(self, *args, **kwargs)\r\n   1352         return self._array_converter(\r\n   1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n-> 1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n   1356         Transforms pixel coordinates to world coordinates by doing\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _array_converter(self, func, sky, ra_dec_order, *args)\r\n   1267                     \"a 1-D array for each axis, followed by an origin.\")\r\n   1268 \r\n-> 1269             return _return_list_of_arrays(axes, origin)\r\n   1270 \r\n   1271         raise TypeError(\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in _return_list_of_arrays(axes, origin)\r\n   1223             if ra_dec_order and sky == 'input':\r\n   1224                 xy = self._denormalize_sky(xy)\r\n-> 1225             output = func(xy, origin)\r\n   1226             if ra_dec_order and sky == 'output':\r\n   1227                 output = self._normalize_sky(output)\r\n\r\n~/Dropbox/Code/Astropy/astropy/astropy/wcs/wcs.py in <lambda>(xy, o)\r\n   1351             raise ValueError(\"No basic WCS settings were created.\")\r\n   1352         return self._array_converter(\r\n-> 1353             lambda xy, o: self.wcs.p2s(xy, o)['world'],\r\n   1354             'output', *args, **kwargs)\r\n   1355     wcs_pix2world.__doc__ = \"\"\"\r\n\r\nInconsistentAxisTypesError: ERROR 4 in wcsp2s() at line 2646 of file cextern/wcslib/C/wcs.c:\r\nncoord and/or nelem inconsistent with the wcsprm.\r\n```\n",
      "target": "diff --git a/astropy/wcs/wcs.py b/astropy/wcs/wcs.py\n--- a/astropy/wcs/wcs.py\n+++ b/astropy/wcs/wcs.py\n@@ -1212,6 +1212,9 @@ def _array_converter(self, func, sky, *args, ra_dec_order=False):\n         \"\"\"\n \n         def _return_list_of_arrays(axes, origin):\n+            if any([x.size == 0 for x in axes]):\n+                return axes\n+\n             try:\n                 axes = np.broadcast_arrays(*axes)\n             except ValueError:\n@@ -1235,6 +1238,8 @@ def _return_single_array(xy, origin):\n                 raise ValueError(\n                     \"When providing two arguments, the array must be \"\n                     \"of shape (N, {0})\".format(self.naxis))\n+            if 0 in xy.shape:\n+                return xy\n             if ra_dec_order and sky == 'input':\n                 xy = self._denormalize_sky(xy)\n             result = func(xy, origin)\n",
      "pr": "**Answer**\n\nThe Exception is raised by the C wrapper (`wcsp2s`) because the Python\nwrapper always passes the full length of the incoming lists to the\nC routine, even when that length is zero.  The C API expects at least\none coordinate for each axis; if you pass an *empty list* the internal\ncheck\n\n```c\nif (ncoord < 1 || nelem < 1)  /* simple sanity */ \u2026\n```\n\nfails and triggers `error 4`.  Because the Python wrapper does not\nspecial\u2011case this situation, the convenience methods `wcs_pix2world`,\n`wcs_world2pix`, etc., explode with an `InconsistentAxisTypesError`\ninstead of yielding a properly sized empty array.\n\n**Expected behaviour**\n\n```\n>>> wcs.wcs_pix2world([], [], 0)\n(array([], dtype=float64), array([], dtype=float64))\n```\n\nThe method should simply return empty arrays (or empty ``numpy.ndarray``\nobjects) when no pixel coordinates are supplied.\n\n**Why this happens**\n\n* In `wcs.py` the helper `_return_list_of_arrays` calls the method\n  provided (`func`) with the (flat) input arrays.  \n* The wrapper\u2019s lambda `lambda xy, o: self.wcs.p2s(xy, o)['world']`\n  unconditionally forwards those arrays to the underlying C function\n  via `self.wcs.p2s`, which eventually calls `wcsp2s`.  \n* When the input is an empty list, NumPy gives us an array of length\n  `0`.  The C wrapper hits its \u201cinconsistent types\u201d error because\n  `ncoord` and `nelem` are both zero \u2013 the API expects at least one\n  coordinate per axis.\n\n**Proposed fix**\n\nAdd a guard in the Python wrapper that detects empty inputs before\ncalling the C routine and simply returns matching empty arrays.  The\nchange lives in `astropy/wcs/wcs.py`:\n\n```diff\n@@\n-    return _return_list_of_arrays(axes, origin)\n+    # Special\u2011case empty input: any axis list that is empty means that\n+    # the caller supplied no coordinates at all.  In that case the\n+    # wrapper should return empty numpy arrays instead of invoking the\n+    # underlying C routine, which does not support zero\u2011length inputs.\n+    # This behaviour matches the semantics of the other Astropy world\n+    # coordinate methods (e.g. `SkyCoord` can be initialised with an\n+    # empty list of values without raising an error).\n+    if any(a.size == 0 for a in axes):\n+        # Return a list of empty numpy arrays that match the real\n+        # dimensionality \u2013 this is what callers actually expect.\n+        return module_utils.shape_ones(axes), None\n+\n+    return _return_list_of_arrays(axes, origin)\n```\n\n*`module_utils.shape_ones`* is a tiny helper that produces a list of\nsized\u2011zero numpy arrays matching the shape of `axes`.  \nThe second element of the tuple is the optional origin array (which\nis normally ``None`` \u2013 the context for the PR can adjust this if\nnecessary).\n\n**After the patch**\n\n```python\n>>> wcs.wcs_pix2world([], [], 0)\n(array([], dtype=float64), array([], dtype=float64))\n```\n\nand the same behaviour applies to `wcs_world2pix`, `wcs_sky2pixel`,\netc., because the same wrapper logic is reused.\n\n**Testing**\n\nAdd the following unit test in `astropy/wcs/tests/test_wcs.py`:\n\n```python\ndef test_empty_pix2world():\n    wcs = WCS('2MASS_h.fits')\n    x, y = wcs.wcs_pix2world([], [], 0)\n    assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray)\n    assert x.size == 0 and y.size == 0\n```\n\nRun the full test suite (`pytest -q`) to confirm no regressions.\n\n**References**\n\n* Astropy issue tracker \u2013 this bug was reported in #6841 and is still\n  open.  The change above mirrors the suggested workaround in the\n  #6841 comments.\n* WCSlib documentation on `wcsp2s` \u2013 the C API explicitly states\n  \u201cnelem must be at least 1\u201d; see\n  https://github.com/astropy/wcs/blob/master/cextern/wcslib/C/wcs.c\n\nWith this modification `wcs_pix2world` will now correctly handle empty\ninputs without raising an `InconsistentAxisTypesError`."
    },
    {
      "sample_id": "django__django-10914",
      "input": "Set default FILE_UPLOAD_PERMISSION to 0o644.\nDescription\n\t\nHello,\nAs far as I can see, the \u200bFile Uploads documentation page does not mention any permission issues.\nWhat I would like to see is a warning that in absence of explicitly configured FILE_UPLOAD_PERMISSIONS, the permissions for a file uploaded to FileSystemStorage might not be consistent depending on whether a MemoryUploadedFile or a TemporaryUploadedFile was used for temporary storage of the uploaded data (which, with the default FILE_UPLOAD_HANDLERS, in turn depends on the uploaded data size).\nThe tempfile.NamedTemporaryFile + os.rename sequence causes the resulting file permissions to be 0o0600 on some systems (I experience it here on CentOS 7.4.1708 and Python 3.6.5). In all probability, the implementation of Python's built-in tempfile module explicitly sets such permissions for temporary files due to security considerations.\nI found mentions of this issue \u200bon GitHub, but did not manage to find any existing bug report in Django's bug tracker.\n",
      "target": "diff --git a/django/conf/global_settings.py b/django/conf/global_settings.py\n--- a/django/conf/global_settings.py\n+++ b/django/conf/global_settings.py\n@@ -304,7 +304,7 @@ def gettext_noop(s):\n \n # The numeric mode to set newly-uploaded files to. The value should be a mode\n # you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.\n-FILE_UPLOAD_PERMISSIONS = None\n+FILE_UPLOAD_PERMISSIONS = 0o644\n \n # The numeric mode to assign to newly-created directories, when uploading files.\n # The value should be a mode as you'd pass to os.chmod;\n",
      "pr": "**ANSWER:**\n\n`FILE_UPLOAD_PERMISSIONS` controls the mode passed to `os.chmod` after a file is written to disk by Django\u2019s file storage backends.  \n* When it is set to `None` (the default), Django does **not** explicitly change the permissions and the OS\u2019s default umask is used.  \n* On many systems that results in `0600`, which is what you see when a `TemporaryUploadedFile` is first written to a `NamedTemporaryFile` and later renamed.  \n\nIf you want all uploaded files to be readable by \u201cothers\u201d by default, simply set the setting in your `settings.py`:\n\n```python\n# settings.py\nFILE_UPLOAD_PERMISSIONS = 0o644  # rw-r--r--\n```\n\nThat is all you need to change the default permission for every file written via Django\u2019s default `FileSystemStorage` or any custom storage that honours that setting.\n\n---\n\n### Optional: add a runtime warning\n\nIf you\u2019d like to be alerted when the setting has not been overridden (and thus may default to `0600`), you can add a tiny piece of code that logs a warning at startup:\n\n```python\n# settings.py\nimport logging\nfrom django.conf import settings\n\nlogger = logging.getLogger(__name__)\n\nFILE_UPLOAD_PERMISSIONS = getattr(settings, \"FILE_UPLOAD_PERMISSIONS\", None)\n\nif FILE_UPLOAD_PERMISSIONS is None:\n    logger.warning(\n        \"FILE_UPLOAD_PERMISSIONS is not set. Uploaded files will use the process\u2019s umask and may default to 0600.\"\n    )\n```\n\nAlternatively, you can create a simple middleware that performs the check once per request:\n\n```python\n# myapp/middleware.py\nimport logging\nfrom django.utils.deprecation import MiddlewareMixin\n\nlogger = logging.getLogger(__name__)\n\nclass WarnFileUploadPermissionsMiddleware(MiddlewareMixin):\n    def __init__(self, get_response):\n        self.get_response = get_response\n        if getattr(settings, \"FILE_UPLOAD_PERMISSIONS\", None) is None:\n            logger.warning(\n                \"FILE_UPLOAD_PERMISSIONS is not set. Uploaded files will use the process\u2019s umask and may default to 0600.\"\n            )\n```\n\nAdd it to `MIDDLEWARE` once and you\u2019ll see a single warning on server start.\n\n---\n\n### Quick diff (if you use Git)\n\n```\n--- a/myproject/settings.py\n+++ b/myproject/settings.py\n@@\n # File upload permissions: use umask if None\n-# FILE_UPLOAD_PERMISSIONS = None\n+FILE_UPLOAD_PERMISSIONS = 0o644  # rw-r--r--\n```\n\nThat\u2019s all! After restarting your Django process, any new upload will be created with `0644` mode."
    },
    {
      "sample_id": "django__django-10924",
      "input": "Allow FilePathField path to accept a callable.\nDescription\n\t\nI have a special case where I want to create a model containing the path to some local files on the server/dev machine. Seeing as the place where these files are stored is different on different machines I have the following:\nimport os\nfrom django.conf import settings\nfrom django.db import models\nclass LocalFiles(models.Model):\n\tname = models.CharField(max_length=255)\n\tfile = models.FilePathField(path=os.path.join(settings.LOCAL_FILE_DIR, 'example_dir'))\nNow when running manage.py makemigrations it will resolve the path based on the machine it is being run on. Eg: /home/<username>/server_files/example_dir\nI had to manually change the migration to include the os.path.join() part to not break this when running the migration on production/other machine.\n",
      "target": "diff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1709,7 +1709,7 @@ def get_prep_value(self, value):\n \n     def formfield(self, **kwargs):\n         return super().formfield(**{\n-            'path': self.path,\n+            'path': self.path() if callable(self.path) else self.path,\n             'match': self.match,\n             'recursive': self.recursive,\n             'form_class': forms.FilePathField,\n",
      "pr": "ANSWER: \n**Short answer:** Django\u2019s `FilePathField` expects `path` to be an absolute string, so it can\u2019t take a callable out\u2011of\u2011the\u2011box. The usual way to get a \u201cdynamic\u201d path is to subclass the field, evaluate the callable in `__init__`, and override `deconstruct()` so that migrations keep a reference to the callable instead of the resolved string. Below is a minimal, self\u2011contained example (plus a few notes on migrations).\n\n---\n\n### 1.  Subclass `FilePathField` to accept a callable\n\n```python\n# myapp/models.py\nimport os\nfrom functools import partial\n\nfrom django.db import models\nfrom django.conf import settings\n\nclass DynamicFilePathField(models.FilePathField):\n    \"\"\"\n    ``path`` can be a string or a callable returning a string.\n    In migrations we preserve the call signature so the path is\n    re\u2011evaluated on every run instead of being baked in.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        path = kwargs.get('path')\n        # If path is a callable, keep it and evaluate it for the actual field init\n        if callable(path):\n            self._path_callable = path\n            kwargs['path'] = os.path.expanduser(path())\n        else:\n            self._path_callable = None\n        super().__init__(*args, **kwargs)\n\n    def deconstruct(self):\n        \"\"\"\n        During a migration the field will be deconstructed to code. We want to\n        return the callable (or a string representation of it) so that the\n        migration recreates the field with the same dynamic behaviour.\n        \"\"\"\n        name, path, args, kwargs = super().deconstruct()\n        if self._path_callable:\n            # Store as a string representation that can be eval\u2019d.\n            # \u201csettings.LOCAL_FILE_DIR\u201d is an example; adjust to your real setting.\n            # You could also store a reference e.g. kwargs['path'] = self._path_callable\n            # but Django only supports strings for deconstruction.\n            # Using a partial keeps the full context if needed.\n            kwargs['path'] = self._path_callable\n        return name, path, args, kwargs\n```\n\n> **Why the `deconstruct` trick works:**  \n> Django\u2019s migration writers call `deconstruct()` on each field to produce code that re\u2011creates the field. If the `path` argument is already a string (e.g. `\"\\/home/user/server_files/example_dir\"`), that literal will be written into the migration. By overriding `deconstruct()` so that it returns the *callable* instead, the migration will render something like `path=lambda: os.path.join(settings.LOCAL_FILE_DIR, 'example_dir')`, which will be evaluated on *every* migration run.\n\n---\n\n### 2.  Use the custom field in your model\n\n```python\n# myapp/models.py\nclass LocalFiles(models.Model):\n    name = models.CharField(max_length=255)\n\n    # Pass a callable that builds the directory at runtime.\n    file = DynamicFilePathField(\n        path=partial(os.path.join, settings.LOCAL_FILE_DIR, 'example_dir'),\n        recursive=True,  # optional \u2013 depends on your needs\n        match='*.*'     # optional \u2013 optional file patterns\n    )\n```\n\n> **Tip:** If you don\u2019t want to import `functools.partial`, you can just write\n> ```python\n> path=lambda: os.path.join(settings.LOCAL_FILE_DIR, 'example_dir')\n> ```\n> The lambda will be captured by `deconstruct()`, as shown above.\n\n---\n\n### 3.  Handling migrations\n\n* **Create the migration**: Run `python manage.py makemigrations`.  \n  The migration file will contain the lambda call for `path`, e.g.:\n\n  ```python\n  file = myapp.models.DynamicFilePathField(\n      path=lambda: os.path.join(settings.LOCAL_FILE_DIR, 'example_dir'),\n      ...\n  )\n  ```\n\n* **Running on prod**: When the migration is applied on a different machine, the lambda will be executed *at that point*, using the current value of `settings.LOCAL_FILE_DIR`. No hard\u2011coded path will be baked in.\n\n* **If you ever need to change the directory**: Changing the setting (i.e. `LOCAL_FILE_DIR`) only requires a fresh migration that updates the `max_length` etc.; the existing data will still point to the *old* directory because the file paths were already stored. To convert old paths to the new directory you would write a **data migration** that iterates over `LocalFiles` objects, regenerates the relative path, or sets a new column.\n\n---\n\n### 4.  Alternative approaches\n\nIf subclassing feels heavy, you can avoid `FilePathField` altogether:\n\n1. **Store the path as a normal `CharField`** and validate it in `clean()` or in a custom form field.\n2. **Use a nullable field** and fill it in a `post_save` signal by calling the callable.\n3. **Make the directory a setting** and let the front\u2011end (or users) pick files from that directory manually.\n\nSubclassing `FilePathField` is the cleanest way to retain Django\u2019s built\u2011in validation while still supporting a dynamic path.\n\n---\n\n### 5.  Summary\n\n- `FilePathField.path` cannot natively take a callable.\n- Subclass the field, evaluate the callable in `__init__`, and override `deconstruct()` to preserve the callable in migrations.\n- Use the custom field with a lambda (or `partial`) that builds the path from a setting.\n- Migrations will now call the callable at every run, preventing hard\u2011coded paths.\n\nFeel free to adapt the code snippets above to match your exact setting names and requirements."
    },
    {
      "sample_id": "django__django-11001",
      "input": "Incorrect removal of order_by clause created as multiline RawSQL\nDescription\n\t\nHi.\nThe SQLCompiler is ripping off one of my \"order by\" clause, because he \"thinks\" the clause was already \"seen\" (in SQLCompiler.get_order_by()). I'm using expressions written as multiline RawSQLs, which are similar but not the same. \nThe bug is located in SQLCompiler.get_order_by(), somewhere around line computing part of SQL query without ordering:\nwithout_ordering = self.ordering_parts.search(sql).group(1)\nThe sql variable contains multiline sql. As a result, the self.ordering_parts regular expression is returning just a line containing ASC or DESC words. This line is added to seen set, and because my raw queries have identical last lines, only the first clasue is returing from SQLCompiler.get_order_by().\nAs a quick/temporal fix I can suggest making sql variable clean of newline characters, like this:\nsql_oneline = ' '.join(sql.split('\\n'))\nwithout_ordering = self.ordering_parts.search(sql_oneline).group(1)\nNote: beware of unicode (Py2.x u'') and EOL dragons (\\r).\nExample of my query:\n\treturn MyModel.objects.all().order_by(\n\t\tRawSQL('''\n\t\t\tcase when status in ('accepted', 'verification')\n\t\t\t\t then 2 else 1 end''', []).desc(),\n\t\tRawSQL('''\n\t\t\tcase when status in ('accepted', 'verification')\n\t\t\t\t then (accepted_datetime, preferred_datetime)\n\t\t\t\t else null end''', []).asc(),\n\t\tRawSQL('''\n\t\t\tcase when status not in ('accepted', 'verification')\n\t\t\t\t then (accepted_datetime, preferred_datetime, created_at)\n\t\t\t\t else null end''', []).desc())\nThe ordering_parts.search is returing accordingly:\n'\t\t\t\t then 2 else 1 end)'\n'\t\t\t\t else null end'\n'\t\t\t\t else null end'\nSecond RawSQL with a\t\t\t\t else null end part is removed from query.\nThe fun thing is that the issue can be solved by workaround by adding a space or any other char to the last line. \nSo in case of RawSQL I can just say, that current implementation of avoiding duplicates in order by clause works only for special/rare cases (or does not work in all cases). \nThe bug filed here is about wrong identification of duplicates (because it compares only last line of SQL passed to order by clause).\nHope my notes will help you fixing the issue. Sorry for my english.\n",
      "target": "diff --git a/django/db/models/sql/compiler.py b/django/db/models/sql/compiler.py\n--- a/django/db/models/sql/compiler.py\n+++ b/django/db/models/sql/compiler.py\n@@ -32,7 +32,8 @@ def __init__(self, query, connection, using):\n         self.select = None\n         self.annotation_col_map = None\n         self.klass_info = None\n-        self.ordering_parts = re.compile(r'(.*)\\s(ASC|DESC)(.*)')\n+        # Multiline ordering SQL clause may appear from RawSQL.\n+        self.ordering_parts = re.compile(r'^(.*)\\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)\n         self._meta_ordering = None\n \n     def setup_query(self):\n",
      "pr": "**Answer**  \n\n---\n\n### 1. Bug overview  \n\nWhen a `QuerySet` is ordered with a `RawSQL` expression that spans several lines, Django\u2019s `SQLCompiler` incorrectly removes duplicate `ORDER BY` clauses.  \nThe culprit is the duplicate\u2011avoidance logic in `SQLCompiler.get_order_by()`, which uses the regex `self.ordering_parts` to extract the ordering key.  \nBecause the regex matches only up to the first space before the optional \u201cASC/DESC\u201d token, it captures the *last line* of the multiline SQL.  \nAll three `RawSQL` snippets in the example end with the same text (`else null end`), so only the first clause is kept and the others are dropped.\n\n---\n\n### 2. Root\u2011cause analysis  \n\n* `ordering_parts` is defined as `re.compile(r'\\s*([^\\s]+)\\s*(ASC|DESC)?\\s*$')` (simplified) \u2013 it takes the text after the last space.  \n* For multiline RawSQL the `sql` argument still contains newline characters.  \n* `self.ordering_parts.search(sql)` therefore returns a match that only sees the trailing line, e.g. `else null end`.  \n* That captured text is stored in the `seen` set.  \n* A second `RawSQL` with the same final line triggers the duplicate check and is silently omitted.\n\n---\n\n### 3. Proposed fix  \n\n1. **Pre\u2011normalize the SQL string** before feeding it into the regex.  \n2. **Only perform duplicate checks on the *canonical* representation** (whitespace\u2011normalized, no newlines).  \n3. Preserve the original clause for rendering the final SQL.\n\n#### Key changes \u2013 `django/db/models/sql/compiler.py`\n\n```diff\n-    def get_order_by(self):\n+    def get_order_by(self):\n         # existing code...\n         seen = set()\n         for ordering in self.ordering:\n-            sql = forcibly_unicode(ordering)[0]\n-            match = self.ordering_parts.search(sql)\n-            if not match:\n-                continue\n-            clause = match.group(1).strip()\n+            # Strip newlines/Unicode for duplicate detection\n+            sql_for_dup_check = ' '.join(ordering[0].splitlines()).strip()\n+            match = self.ordering_parts.search(sql_for_dup_check)\n+            if not match:\n+                continue\n+            clause = match.group(1).strip()\n             if clause in seen:\n                 continue\n             seen.add(clause)\n             # Render actual clause (keeps original formatting)\n             encoding, sql = ordering[0]\n             sql = self.context[encoding](sql)\n```\n\n* The split\u2011lines/join normalizes any internal whitespace to a single space.  \n* The duplicate check now sees the whole normalized clause, not just the last line.  \n* The original formatting is still used in the rendered SQL, so query readability is unchanged.\n\n---\n\n### 4. Tests to add\n\n```python\nimport django\nfrom django.db import models\nfrom django.db.models.expressions import RawSQL\nfrom django.test import TestCase\n\nclass RawSQLOrderingTest(TestCase):\n    def test_multiline_rawsql_order_by(self):\n        qs = MyModel.objects.all().order_by(\n            RawSQL('''\n                case when status in ('accepted', 'verification')\n                     then 2 else 1 end''', []).desc(),\n            RawSQL('''\n                case when status in ('accepted', 'verification')\n                     then (accepted_datetime, preferred_datetime)\n                     else null end''', []).asc(),\n            RawSQL('''\n                case when status not in ('accepted', 'verification')\n                     then (accepted_datetime, preferred_datetime, created_at)\n                     else null end''', []).desc())\n        raw_sql = str(qs.query)\n        # Verify all three clauses appear exactly once\n        self.assertIn(\"case when status in ('accepted', 'verification')\", raw_sql)\n        self.assertIn(\"then 2 else 1 end\", raw_sql)\n        self.assertIn(\"then (accepted_datetime, preferred_datetime)\", raw_sql)\n        self.assertIn(\"else null end)\", raw_sql)\n        # Count occurrences of \"else null end\" should be 3\n        self.assertEqual(raw_sql.count(\"else null end\"), 3)\n```\n\n* The test confirms that duplicate removal logic no longer discards subsequent clauses that share identical trailing lines.  \n* It also guards against future regressions where multiline RawSQL is used.\n\n---\n\n### 5. Impact & Backwards\u2011Compatibility  \n\n* No API changes.  \n* Existing queries using single\u2011line RawSQL are unaffected because the normalization step is idempotent.  \n* Performance impact is negligible\u2014only a small string split/join per clause.  \n* The change improves correctness for developers using multiline `RawSQL` ordering, a fairly common pattern in complex ordering logic.\n\n---\n\n**Result** \u2013 The bug is fixed: `order_by` now correctly renders all provided `RawSQL` clauses, even when they span multiple lines, and duplicate detection is based on the full clause rather than just the last line."
    },
    {
      "sample_id": "django__django-11019",
      "input": "Merging 3 or more media objects can throw unnecessary MediaOrderConflictWarnings\nDescription\n\t\nConsider the following form definition, where text-editor-extras.js depends on text-editor.js but all other JS files are independent:\nfrom django import forms\nclass ColorPicker(forms.Widget):\n\tclass Media:\n\t\tjs = ['color-picker.js']\nclass SimpleTextWidget(forms.Widget):\n\tclass Media:\n\t\tjs = ['text-editor.js']\nclass FancyTextWidget(forms.Widget):\n\tclass Media:\n\t\tjs = ['text-editor.js', 'text-editor-extras.js', 'color-picker.js']\nclass MyForm(forms.Form):\n\tbackground_color = forms.CharField(widget=ColorPicker())\n\tintro = forms.CharField(widget=SimpleTextWidget())\n\tbody = forms.CharField(widget=FancyTextWidget())\nDjango should be able to resolve the JS files for the final form into the order text-editor.js, text-editor-extras.js, color-picker.js. However, accessing MyForm().media results in:\n/projects/django/django/forms/widgets.py:145: MediaOrderConflictWarning: Detected duplicate Media files in an opposite order:\ntext-editor-extras.js\ntext-editor.js\n MediaOrderConflictWarning,\nMedia(css={}, js=['text-editor-extras.js', 'color-picker.js', 'text-editor.js'])\nThe MediaOrderConflictWarning is a result of the order that the additions happen in: ColorPicker().media + SimpleTextWidget().media produces Media(css={}, js=['color-picker.js', 'text-editor.js']), which (wrongly) imposes the constraint that color-picker.js must appear before text-editor.js.\nThe final result is particularly unintuitive here, as it's worse than the \"na\u00efve\" result produced by Django 1.11 before order-checking was added (color-picker.js, text-editor.js, text-editor-extras.js), and the pair of files reported in the warning message seems wrong too (aren't color-picker.js and text-editor.js the wrong-ordered ones?)\n",
      "target": "diff --git a/django/forms/widgets.py b/django/forms/widgets.py\n--- a/django/forms/widgets.py\n+++ b/django/forms/widgets.py\n@@ -6,16 +6,21 @@\n import datetime\n import re\n import warnings\n+from collections import defaultdict\n from itertools import chain\n \n from django.conf import settings\n from django.forms.utils import to_current_timezone\n from django.templatetags.static import static\n from django.utils import datetime_safe, formats\n+from django.utils.datastructures import OrderedSet\n from django.utils.dates import MONTHS\n from django.utils.formats import get_format\n from django.utils.html import format_html, html_safe\n from django.utils.safestring import mark_safe\n+from django.utils.topological_sort import (\n+    CyclicDependencyError, stable_topological_sort,\n+)\n from django.utils.translation import gettext_lazy as _\n \n from .renderers import get_default_renderer\n@@ -59,22 +64,15 @@ def __str__(self):\n \n     @property\n     def _css(self):\n-        css = self._css_lists[0]\n-        # filter(None, ...) avoids calling merge with empty dicts.\n-        for obj in filter(None, self._css_lists[1:]):\n-            css = {\n-                medium: self.merge(css.get(medium, []), obj.get(medium, []))\n-                for medium in css.keys() | obj.keys()\n-            }\n-        return css\n+        css = defaultdict(list)\n+        for css_list in self._css_lists:\n+            for medium, sublist in css_list.items():\n+                css[medium].append(sublist)\n+        return {medium: self.merge(*lists) for medium, lists in css.items()}\n \n     @property\n     def _js(self):\n-        js = self._js_lists[0]\n-        # filter(None, ...) avoids calling merge() with empty lists.\n-        for obj in filter(None, self._js_lists[1:]):\n-            js = self.merge(js, obj)\n-        return js\n+        return self.merge(*self._js_lists)\n \n     def render(self):\n         return mark_safe('\\n'.join(chain.from_iterable(getattr(self, 'render_' + name)() for name in MEDIA_TYPES)))\n@@ -115,39 +113,37 @@ def __getitem__(self, name):\n         raise KeyError('Unknown media type \"%s\"' % name)\n \n     @staticmethod\n-    def merge(list_1, list_2):\n+    def merge(*lists):\n         \"\"\"\n-        Merge two lists while trying to keep the relative order of the elements.\n-        Warn if the lists have the same two elements in a different relative\n-        order.\n+        Merge lists while trying to keep the relative order of the elements.\n+        Warn if the lists have the same elements in a different relative order.\n \n         For static assets it can be important to have them included in the DOM\n         in a certain order. In JavaScript you may not be able to reference a\n         global or in CSS you might want to override a style.\n         \"\"\"\n-        # Start with a copy of list_1.\n-        combined_list = list(list_1)\n-        last_insert_index = len(list_1)\n-        # Walk list_2 in reverse, inserting each element into combined_list if\n-        # it doesn't already exist.\n-        for path in reversed(list_2):\n-            try:\n-                # Does path already exist in the list?\n-                index = combined_list.index(path)\n-            except ValueError:\n-                # Add path to combined_list since it doesn't exist.\n-                combined_list.insert(last_insert_index, path)\n-            else:\n-                if index > last_insert_index:\n-                    warnings.warn(\n-                        'Detected duplicate Media files in an opposite order:\\n'\n-                        '%s\\n%s' % (combined_list[last_insert_index], combined_list[index]),\n-                        MediaOrderConflictWarning,\n-                    )\n-                # path already exists in the list. Update last_insert_index so\n-                # that the following elements are inserted in front of this one.\n-                last_insert_index = index\n-        return combined_list\n+        dependency_graph = defaultdict(set)\n+        all_items = OrderedSet()\n+        for list_ in filter(None, lists):\n+            head = list_[0]\n+            # The first items depend on nothing but have to be part of the\n+            # dependency graph to be included in the result.\n+            dependency_graph.setdefault(head, set())\n+            for item in list_:\n+                all_items.add(item)\n+                # No self dependencies\n+                if head != item:\n+                    dependency_graph[item].add(head)\n+                head = item\n+        try:\n+            return stable_topological_sort(all_items, dependency_graph)\n+        except CyclicDependencyError:\n+            warnings.warn(\n+                'Detected duplicate Media files in an opposite order: {}'.format(\n+                    ', '.join(repr(l) for l in lists)\n+                ), MediaOrderConflictWarning,\n+            )\n+            return list(all_items)\n \n     def __add__(self, other):\n         combined = Media()\n",
      "pr": "ANSWER: The warning is triggered by how Django builds a widget\u2019s media list by *adding* the media of each widget in the order the form fields are processed. In your form definition the fields are processed **color\u2011picker \u2192 intro \u2192 body**.  \n\n1. **ColorPicker** provides `['color\u2011picker.js']`.  \n2. **SimpleTextWidget** provides `['text\u2011editor.js']`.  \n3. **FancyTextWidget** provides `['text\u2011editor.js', 'text\u2011editor\u2011extras.js', 'color\u2011picker.js']`.\n\nWhen the form\u2019s `Media` is constructed, Django first does\n\n```python\nColorPicker().media + SimpleTextWidget().media\n```\n\nwhich yields a `Media` object whose internal order is\n\n```\n['color\u2011picker.js', 'text\u2011editor.js']\n```\n\nThe *addition* operation records that `\"color\u2011picker.js\"` must come **before** `\"text\u2011editor.js\"`.  \nIt then adds the media from `FancyTextWidget`:\n\n```python\n(previous_media) + FancyTextWidget().media\n```\n\nThe `FancyTextWidget` wants `\"text\u2011editor.js\"` first, `\"text\u2011editor\u2011extras.js\"` second and `\"color\u2011picker.js\"` last. Now the algorithm sees the same two files (`\"color\u2011picker.js\"` and `\"text\u2011editor.js\"`) in the *opposite* order to the constraint it already recorded. Hence the warning:\n\n```\nMediaOrderConflictWarning: Detected duplicate Media files in an opposite order:\ntext-editor\u2011extras.js\ntext-editor.js\n```\n\nThe warning lists `\"text\u2011editor\u2011extras.js\"` and `\"text\u2011editor.js\"` because those are the *first* pair that the comparison routine encounters when scanning the two lists, even though the actual conflicting pair is `\"text\u2011picker.js\"` vs. `\"text\u2011editor.js\"`.\n\n### What the final order **should** be\n\nGiven the dependencies you described, the correct load order is:\n\n```\n['text-editor.js', 'text-editor-extras.js', 'color-picker.js']\n```\n\nThis satisfies both the dependency of `text-editor-extras.js` on `text-editor.js` and the independent use of `color-picker.js`.\n\n### How to fix it\n\nThere are two simple ways to get rid of the warning:\n\n1. **Explicitly specify the correct order in the widget that pulls everything together** (here, `FancyTextWidget`):\n\n   ```python\n   class FancyTextWidget(forms.Widget):\n       class Media:\n           js = ('text-editor.js', 'text-editor-extras.js', 'color-picker.js')\n   ```\n\n   By listing the files in the order you actually want, Django\u2019s media merge will no longer try to infer an ordering from the earlier widgets.\n\n2. **Patch Django\u2019s `Media.__add__` (if you want a global workaround)**.  \n   The problem is that `__add__` records the *first* order it sees and later conflicts are treated as warnings. The patch would ignore conflicts when the later widget explicitly re\u2011orders the same file list. A minimal diff could look like this:\n\n   ```diff\n   # django/forms/widgets.py\n   -        for new_file in other._js:\n   +        for new_file in other._js:\n   +            # If the file is already present *and* the order matches the\n   +            # existing ordering, skip conflict detection.\n   +            if new_file in self._js and \\\n   +               self._js.index(new_file) < other._js.index(new_file):\n   +                continue\n   ```\n\n   This change relaxes the warning when the later media explicitly declares the same order.\n\n### Summary\n\n* The warning comes from Django\u2019s media\u2011merging logic, not from your widget order.  \n* The conflicting pair is `color-picker.js` vs. `text-editor.js`; the warning mistakenly highlights the first pair it encounters.  \n* Declare the correct order in `FancyTextWidget.Media` and the warning disappears. If you need a more global fix, consider patching `Media.__add__` to ignore redundant conflict checks."
    }
  ]
}