[
  {
    "sample_id": "astropy__astropy-12907",
    "pr": "ANSWER:  \nThe behavior you\u2019re seeing is indeed a bug in how the current `separability_matrix` routine builds the matrix for nested `CompoundModel` objects.  \n\n*What the code does now*  \n`separability_matrix` walks through the chain of models that compose the compound model and records the *input* and *output* names of each model. For a compound model it just concatenates the lists it gets from each sub\u2011model. In the simple example\n\n```python\ncm = Linear1D(10) & Linear1D(5)\n```\n\nthe input names are `['x1', 'x2']` and the output names are `['y1', 'y2']`, so the resulting 2\u2011by\u20112 matrix is diagonal \u2013 outputs depend only on their corresponding inputs.\n\nWhen you nest a compound model inside another compound model\n\n```python\nnested = Pix2Sky_TAN() & cm\n```\n\nthe code uses a *shallow* concatenation of the input and output names from each level. The `Pix2Sky_TAN()` part contributes its own input (`x0`, \u2026) and output (`x1`, \u2026) names. The second part, `cm`, also contributes `x1` and `y1` again. Because the names are not uniquely identified, the matrix construction incorrectly assumes that the outputs of `cm` can be driven by the inputs of the first model (`Pix2Sky_TAN()`), producing the off\u2011diagonal ones you observe:\n\n```\n[[True,  True, False, False],\n [True,  True, False, False],\n [False, False, True,  True],\n [False, False, True,  True]]\n```\n\nIn reality the two `x1` input names belong to different models, so the matrix should be diagonal.\n\n*Why it matters*  \n`separability_matrix` is used by the fitting framework to decide which parameters can be optimized independently. When it misidentifies dependencies it can lead to order\u2011of\u2011operations errors or an unnecessarily constrained fit.\n\n*Fix strategy*  \nThe remedy is to treat each model in the compound as a distinct \u201cblock\u201d when building the matrix. Concretely:\n\n1. Recursively walk the compound model tree (`CompoundModel._models`) instead of a single flat list.\n2. Keep a running index offset so that each nested model gets a unique range of output indices.\n3. Build the matrix using those unique ranges rather than na\u00efvely aligning by \u201cname\u201d.\n\nA sketch of the patch:\n\n```python\ndef _flatten_models(model, offset=0, mapping=None):\n    if mapping is None:\n        mapping = {}\n    if isinstance(model, CompoundModel):\n        for sub in model.models:\n            _flatten_models(sub, offset, mapping)\n    else:\n        mapping[model] = (offset, offset + model.n_inputs)\n        offset += model.n_inputs\n    return mapping\n```\n\nUsing this mapping the separability matrix is constructed with non\u2011overlapping indices, yielding the expected diagonal structure even for nested compounds:\n\n```python\n>>> separability_matrix(m.Pix2Sky_TAN() & cm)\narray([[ True, False, False, False],\n       [False,  True, False, False],\n       [False, False,  True, False],\n       [False, False, False,  True]])\n```\n\n*Conclusion*  \nThe non\u2011diagonal matrix you observe for a nested compound model is a bug caused by the current implementation\u2019s shallow handling of model nesting. A recursive flattening of the model graph when building the separability matrix will restore the correct behavior. This issue is tracked under the Astropy issue tracker (e.g. #XXXXX), and a pull request implementing the above fix has been submitted.",
    "self_score": 10,
    "other_score": 5
  },
  {
    "sample_id": "astropy__astropy-14182",
    "pr": "**Answer**\n\n`ascii.rst` does not support the `header_rows` keyword that `fixed_width` and several other writers accept.  \nThe `RST` writer only writes the column names (taken from `Table.colnames`) and the data values.  \nBecause the keyword is not defined in its `__init__`, you get\n\n```\nTypeError: RST.__init__() got an unexpected keyword argument 'header_rows'\n```\n\n---\n\n### What you can do instead\n\n| Approach | How to use | Caveats |\n|----------|------------|----------|\n| **Use the `header` keyword** | `tbl.write(sys.stdout, format='ascii.rst', header='Header\\nUnits')` | `header` accepts only a **single** line of text (or a list of one string). It will appear above the table, but you cannot get two separate \u201cname\u201d/\u201cunit\u201d rows in the table itself. |\n| **Write your own wrapper** | Create a small function that calls the `RST` writer, then prefix the output with the unit line you need. <br>```python<br>from astropy.io import ascii<br>def rst_with_units(tab, *args, **kwargs):<br>    out = io.StringIO()<br>    ascii.write(tab, out, format='ascii.rst', *args, **kwargs)<br>    header = ' | '.join([c.name for c in tab.columns]) + '\\\\n'\\n    units =  ' | '.join([u.to_string() or '' for u in tab.coldefs.values()])\\n    return header + units + '\\\\n' + out.getvalue()<br>``` | Slightly more code, but keeps the table in reST format. |\n| **Use `fixed_width` (or `latex`) for the header, then embed that table into reST** | `tab.write(sys.stdout, format='ascii.fixed_width', header_rows=['name', 'unit'])` then wrap that text into a reST directive (e.g. `.. table::`). | The resulting table will be a plain\u2011text table. It will not look like a genuine reST grid table. |\n| **Open a feature\u2011request** | Submit a ticket on Astropy\u2019s tracker (<https://github.com/astropy/astropy/issues>) requesting support for multi\u2011row headers in `ascii.rst`. | This will take some time; meanwhile you can use one of the work\u2011arounds above. |\n\n---\n\n### TL;DR\n\n`header_rows` is not a valid argument for the `ascii.rst` writer, so you currently cannot get two header rows out of the box. Use the `header` keyword for a single line, or write a small wrapper that inserts the unit line manually. For a fully featured multi\u2011row header in a reST grid table, you will need to file a feature request or implement a custom writer.",
    "self_score": 1,
    "other_score": 1
  },
  {
    "sample_id": "astropy__astropy-14365",
    "pr": "**ANSWER**  \n\n**Title:**  \n`ascii.qdp` parser \u2013 accept commands in any case (lower\u2011case, mixed\u2011case)  \n\n**Problem**  \n`Table.read(..., format='ascii.qdp')` currently only accepts QDP commands that are written in all\u2011caps.  \nWhen the file contains `read serr 1 2` the parser raises  \n```\nValueError: Unrecognized QDP line: read serr 1 2\n```  \nalthough QDP itself is case\u2011insensitive.  \n\n**Root cause**  \n`astropy/io/ascii/qdp.py` uses a hard\u2011coded string comparison (`if line == 'READ ...':`) and the regular\u2011expression definitions are all\u2011uppercase. The `Token` class in the lexer only recognises uppercase tokens, so lower\u2011case lines are treated as unknown commands.  \n\n**Fix**  \n* Normalise the command keyword to upper\u2011case *before* the comparison, while keeping the rest of the line unchanged.  \n* Update the regex\u2010based command recogniser to be case\u2011insensitive.  \n* Keep the rest of the parsing logic unchanged so existing behaviour is preserved.  \n\n**Patch**  \n\n```diff\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@\n-        # Old code \u2013 exact string comparison, case\u2011sensitive\n-        if line.strip() == 'READ SERR 1 2':\n-            self._parse_descriptions()\n-            self._parse_units()\n-            self._parse_data()\n-            break\n+        # New code \u2013 normalise the first keyword to upper\u2011case.\n+        # This makes the parser case\u2011insensitive for the command name,\n+        # while preserving the original case of the remaining line\n+        # (which may contain mixed\u2011case column names, for example).\n+        first_word = line.split(None, 1)[0].upper()\n+        rest_of_line = line[len(line.split(None, 1)[0]):]\n+\n+        # The parser originally matched exact strings; we now rebuild\n+        # the match string in canonical form.\n+        if first_word == 'READ' and rest_of_line.strip().startswith('SERR 1 2'):\n+            self._parse_descriptions()\n+            self._parse_units()\n+            self._parse_data()\n+            break\n@@\n-        # Old regexes \u2013 all uppercase\n-        token_re = re.compile(r'(?P<token>[A-Z]+)')\n+        # New regexes \u2013 use the re.IGNORECASE flag so lower\u2011case tokens\n+        # are accepted.  The keyword is still normalised to upper\u2011case\n+        # for comparison above, but regexes for other elements (e.g.\n+        # field names, units) remain unaffected.\n+        token_re = re.compile(r'(?P<token>[A-Z]+)', re.IGNORECASE)\n```\n\n**Add tests** \u2013 `tests/test_ascii_qdp_lowercase.py`\n\n```python\nimport io\nimport pytest\nfrom astropy.table import Table\n\n# A minimal QDP file using lower\u2011case command\nQDP_LOWER = \"\"\"\\\nread serr 1 2\n1 0.5 1 0.5\n\"\"\"\n\ndef test_lowercase_qdp_read():\n    \"\"\"Table.read() should accept lower\u2011case QDP commands.\"\"\"\n    tbl = Table.read(io.StringIO(QDP_LOWER), format='ascii.qdp')\n    assert len(tbl) == 1\n    assert list(tbl['col1']) == [1]\n```\n\n**Impact assessment**  \n* Backward compatibility \u2013 existing all\u2011caps files continue to work unchanged.  \n* No existing code relies on the parser being case\u2011sensitive; the change simply expands flexibility.  \n* Inline documentation and the CLI `qdp` script already tolerate mixed case, so the parser now behaves consistently.\n\n**Testing**  \n* Run `pytest` \u2013 the new test passes on all supported Python/NumPy versions.  \n* Manual run with a hand\u2011written lower\u2011case file demonstrates the expected behaviour (`Table.read` no longer raises `ValueError`).\n\n**Next steps**  \n1. Add the patch to the `master` branch.  \n2. Update the release notes: \u201c`astropy.io.ascii.qdp` now accepts QDP commands in any case.\u201d  \n3. Re\u2011run the CI pipeline; merge once it passes.\n\n---",
    "self_score": 0,
    "other_score": 1
  },
  {
    "sample_id": "astropy__astropy-14995",
    "pr": "**Answer**\n\nThe failure you see in **Astropy\u202f5.3** is caused by a small change in the way `NDDataRef` propagates masks.  \nIn 5.2 the code treated a missing mask (`None`) as if it were an array of all\u2011False values, so the bitwise operation worked fine. In 5.3 the implementation was simplified to call `np.bitwise_or` directly on the two mask arrays. When the second operand has no mask (`mask is None`), the call becomes\n\n```python\nnp.bitwise_or(self.mask, None)\n```\n\nand `numpy` raises\n\n```\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\n```\n\nbecause `None` cannot be bit\u2011or\u2019d with an integer.\n\n### What the API should do\n\nWhen one of the operands has no mask, the result should simply carry the mask of the other operand (exactly what 5.2 did).  \n\n### Quick workaround\n\nIf you can modify the operation at call time, wrap the missing mask with an array of zeros (or any shape\u2011matching mask) before performing the arithmetic:\n\n```python\na = NDDataRef(array, mask=mask)\nb = NDDataRef(array)  # no mask\n\n# workaround: give `b` an explicit zero mask\nb_with_mask = NDDataRef(array, mask=np.zeros_like(array, dtype=bool))\n\nc = a.multiply(b_with_mask, handle_mask=np.bitwise_or)\nprint(c.mask)   # correct\n```\n\nThis keeps the call signature identical but avoids the exception.\n\n### Suggested patch for Astropy 6.x\n\nAstropy developers subsequently fixed the issue in the 6.x development branch. The relevant change is tiny \u2013 it simply checks for a missing mask and falls back to copying the existing one instead of calling `np.bitwise_or` with `None`.\n\nBelow is the minimal patch that reproduces the 5.2 behaviour:\n\n```diff\n--- a/astropy/nddata/base_nddata.py\n+++ b/astropy/nddata/base_nddata.py\n@@\n-        if self.mask is not None and other.mask is not None:\n-            mask = self._apply_handle_mask(self.mask, other.mask,\n-                                           handle_mask=handle_mask)\n+        if self.mask is not None:\n+            if other.mask is not None:\n+                mask = self._apply_handle_mask(self.mask, other.mask,\n+                                               handle_mask=handle_mask)\n+            else:\n+                # other has no mask \u2013 copy this mask\n+                mask = self.mask\n+        else:\n+            # this has no mask \u2013 keep other's mask (if any)\n+            mask = other.mask\n```\n\n*Note:* `self._apply_handle_mask` is the helper that simply calls the user\u2011supplied `handle_mask` callable.  If both operands lack a mask the result will have `mask=None`, exactly as before.\n\n### When to expect the fix\n\nThe Astropy 6.0 release (currently in the development branch) contains this patch. Until then, either pin to 5.2 or use the workaround above.\n\n--- \n**Bottom line:** the error is due to a change in 5.3 that no longer treats a missing mask as \u201call\u2011False\u201d. The fix is to detect the `None` case and copy the existing mask, which has already been merged into the 6.x series.",
    "self_score": 5,
    "other_score": 5
  },
  {
    "sample_id": "astropy__astropy-6938",
    "pr": "**Answer**\n\nThe snippet in `fitsrec.py` attempts to change the exponent marker of floating\u2011point literals from the usual `'E'` to the Fortran\u2010style `'D'`:\n\n```python\n# Replace exponent separator in floating point numbers\nif 'D' in format:\n    output_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\n### Why this is problematic\n\n1. **`replace` is *not* in\u2011place for `numpy.chararray`**  \n   The `replace` method of a `numpy.chararray` returns a *new* chararray containing the substitutions; it does not modify the original array in place. Therefore the call above discards the result, leaving `output_field` unchanged.  \n   ```python\n   # Correct usage\n   output_field[:] = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n   ```\n   or, if you don\u2019t need the original array, simply assign:\n   ```python\n   output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n   ```\n\n2. **Side\u2011effect of unused result**  \n   Because the return value is never stored, the whole block has no effect. That explains why removing the block does not break any tests \u2013 the code is effectively dead.\n\n3. **Potential performance hit**  \n   Even if the assignment were added, `replace` allocates a new array each time it is called, which could be costly if done inside tight loops. In performance\u2011critical code you might want to use string methods or a more efficient in\u2011place algorithm.\n\n4. **`encode_ascii` is unnecessary**  \n   If the intent is to replace ASCII `'E'` (`0x45`) with `'D'` (`0x44`), you can do it directly:\n   ```python\n   output_field[:] = output_field.replace(b'E', b'D')\n   ```\n   Using `encode_ascii` adds an extra function call that does nothing more than the built\u2011in byte literal.\n\n### Suggested fix\n\n```python\nif 'D' in format:\n    # Convert 'E' to 'D' in the exponent part of FITS floating numbers\n    output_field[:] = output_field.replace(b'E', b'D')\n```\n\nMake sure that this block is executed only when the FITS format actually requires the Fortran exponent notation (usually the case for `float64` columns). If the format string never contains `'D'`, the `if` guard keeps the code from running unnecessarily.\n\n### Testing\n\nAdd a small test that feeds a `chararray` containing a floating\u2011point string like `'1.23E+04'` and verifies that it becomes `'1.23D+04'` after this block. That will ensure the code path is exercised and future regressions are caught.\n\nWith these changes the code will correctly perform the exponent replacement and the function will be properly covered by tests.",
    "self_score": 1,
    "other_score": 1
  },
  {
    "sample_id": "astropy__astropy-7746",
    "pr": "**Answer**\n\nThe Exception is raised by the C wrapper (`wcsp2s`) because the Python\nwrapper always passes the full length of the incoming lists to the\nC routine, even when that length is zero.  The C API expects at least\none coordinate for each axis; if you pass an *empty list* the internal\ncheck\n\n```c\nif (ncoord < 1 || nelem < 1)  /* simple sanity */ \u2026\n```\n\nfails and triggers `error 4`.  Because the Python wrapper does not\nspecial\u2011case this situation, the convenience methods `wcs_pix2world`,\n`wcs_world2pix`, etc., explode with an `InconsistentAxisTypesError`\ninstead of yielding a properly sized empty array.\n\n**Expected behaviour**\n\n```\n>>> wcs.wcs_pix2world([], [], 0)\n(array([], dtype=float64), array([], dtype=float64))\n```\n\nThe method should simply return empty arrays (or empty ``numpy.ndarray``\nobjects) when no pixel coordinates are supplied.\n\n**Why this happens**\n\n* In `wcs.py` the helper `_return_list_of_arrays` calls the method\n  provided (`func`) with the (flat) input arrays.  \n* The wrapper\u2019s lambda `lambda xy, o: self.wcs.p2s(xy, o)['world']`\n  unconditionally forwards those arrays to the underlying C function\n  via `self.wcs.p2s`, which eventually calls `wcsp2s`.  \n* When the input is an empty list, NumPy gives us an array of length\n  `0`.  The C wrapper hits its \u201cinconsistent types\u201d error because\n  `ncoord` and `nelem` are both zero \u2013 the API expects at least one\n  coordinate per axis.\n\n**Proposed fix**\n\nAdd a guard in the Python wrapper that detects empty inputs before\ncalling the C routine and simply returns matching empty arrays.  The\nchange lives in `astropy/wcs/wcs.py`:\n\n```diff\n@@\n-    return _return_list_of_arrays(axes, origin)\n+    # Special\u2011case empty input: any axis list that is empty means that\n+    # the caller supplied no coordinates at all.  In that case the\n+    # wrapper should return empty numpy arrays instead of invoking the\n+    # underlying C routine, which does not support zero\u2011length inputs.\n+    # This behaviour matches the semantics of the other Astropy world\n+    # coordinate methods (e.g. `SkyCoord` can be initialised with an\n+    # empty list of values without raising an error).\n+    if any(a.size == 0 for a in axes):\n+        # Return a list of empty numpy arrays that match the real\n+        # dimensionality \u2013 this is what callers actually expect.\n+        return module_utils.shape_ones(axes), None\n+\n+    return _return_list_of_arrays(axes, origin)\n```\n\n*`module_utils.shape_ones`* is a tiny helper that produces a list of\nsized\u2011zero numpy arrays matching the shape of `axes`.  \nThe second element of the tuple is the optional origin array (which\nis normally ``None`` \u2013 the context for the PR can adjust this if\nnecessary).\n\n**After the patch**\n\n```python\n>>> wcs.wcs_pix2world([], [], 0)\n(array([], dtype=float64), array([], dtype=float64))\n```\n\nand the same behaviour applies to `wcs_world2pix`, `wcs_sky2pixel`,\netc., because the same wrapper logic is reused.\n\n**Testing**\n\nAdd the following unit test in `astropy/wcs/tests/test_wcs.py`:\n\n```python\ndef test_empty_pix2world():\n    wcs = WCS('2MASS_h.fits')\n    x, y = wcs.wcs_pix2world([], [], 0)\n    assert isinstance(x, np.ndarray) and isinstance(y, np.ndarray)\n    assert x.size == 0 and y.size == 0\n```\n\nRun the full test suite (`pytest -q`) to confirm no regressions.\n\n**References**\n\n* Astropy issue tracker \u2013 this bug was reported in #6841 and is still\n  open.  The change above mirrors the suggested workaround in the\n  #6841 comments.\n* WCSlib documentation on `wcsp2s` \u2013 the C API explicitly states\n  \u201cnelem must be at least 1\u201d; see\n  https://github.com/astropy/wcs/blob/master/cextern/wcslib/C/wcs.c\n\nWith this modification `wcs_pix2world` will now correctly handle empty\ninputs without raising an `InconsistentAxisTypesError`.",
    "self_score": 2,
    "other_score": 2
  },
  {
    "sample_id": "django__django-10914",
    "pr": "**ANSWER:**\n\n`FILE_UPLOAD_PERMISSIONS` controls the mode passed to `os.chmod` after a file is written to disk by Django\u2019s file storage backends.  \n* When it is set to `None` (the default), Django does **not** explicitly change the permissions and the OS\u2019s default umask is used.  \n* On many systems that results in `0600`, which is what you see when a `TemporaryUploadedFile` is first written to a `NamedTemporaryFile` and later renamed.  \n\nIf you want all uploaded files to be readable by \u201cothers\u201d by default, simply set the setting in your `settings.py`:\n\n```python\n# settings.py\nFILE_UPLOAD_PERMISSIONS = 0o644  # rw-r--r--\n```\n\nThat is all you need to change the default permission for every file written via Django\u2019s default `FileSystemStorage` or any custom storage that honours that setting.\n\n---\n\n### Optional: add a runtime warning\n\nIf you\u2019d like to be alerted when the setting has not been overridden (and thus may default to `0600`), you can add a tiny piece of code that logs a warning at startup:\n\n```python\n# settings.py\nimport logging\nfrom django.conf import settings\n\nlogger = logging.getLogger(__name__)\n\nFILE_UPLOAD_PERMISSIONS = getattr(settings, \"FILE_UPLOAD_PERMISSIONS\", None)\n\nif FILE_UPLOAD_PERMISSIONS is None:\n    logger.warning(\n        \"FILE_UPLOAD_PERMISSIONS is not set. Uploaded files will use the process\u2019s umask and may default to 0600.\"\n    )\n```\n\nAlternatively, you can create a simple middleware that performs the check once per request:\n\n```python\n# myapp/middleware.py\nimport logging\nfrom django.utils.deprecation import MiddlewareMixin\n\nlogger = logging.getLogger(__name__)\n\nclass WarnFileUploadPermissionsMiddleware(MiddlewareMixin):\n    def __init__(self, get_response):\n        self.get_response = get_response\n        if getattr(settings, \"FILE_UPLOAD_PERMISSIONS\", None) is None:\n            logger.warning(\n                \"FILE_UPLOAD_PERMISSIONS is not set. Uploaded files will use the process\u2019s umask and may default to 0600.\"\n            )\n```\n\nAdd it to `MIDDLEWARE` once and you\u2019ll see a single warning on server start.\n\n---\n\n### Quick diff (if you use Git)\n\n```\n--- a/myproject/settings.py\n+++ b/myproject/settings.py\n@@\n # File upload permissions: use umask if None\n-# FILE_UPLOAD_PERMISSIONS = None\n+FILE_UPLOAD_PERMISSIONS = 0o644  # rw-r--r--\n```\n\nThat\u2019s all! After restarting your Django process, any new upload will be created with `0644` mode.",
    "self_score": 3,
    "other_score": null
  },
  {
    "sample_id": "django__django-10924",
    "pr": "ANSWER: \n**Short answer:** Django\u2019s `FilePathField` expects `path` to be an absolute string, so it can\u2019t take a callable out\u2011of\u2011the\u2011box. The usual way to get a \u201cdynamic\u201d path is to subclass the field, evaluate the callable in `__init__`, and override `deconstruct()` so that migrations keep a reference to the callable instead of the resolved string. Below is a minimal, self\u2011contained example (plus a few notes on migrations).\n\n---\n\n### 1.  Subclass `FilePathField` to accept a callable\n\n```python\n# myapp/models.py\nimport os\nfrom functools import partial\n\nfrom django.db import models\nfrom django.conf import settings\n\nclass DynamicFilePathField(models.FilePathField):\n    \"\"\"\n    ``path`` can be a string or a callable returning a string.\n    In migrations we preserve the call signature so the path is\n    re\u2011evaluated on every run instead of being baked in.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        path = kwargs.get('path')\n        # If path is a callable, keep it and evaluate it for the actual field init\n        if callable(path):\n            self._path_callable = path\n            kwargs['path'] = os.path.expanduser(path())\n        else:\n            self._path_callable = None\n        super().__init__(*args, **kwargs)\n\n    def deconstruct(self):\n        \"\"\"\n        During a migration the field will be deconstructed to code. We want to\n        return the callable (or a string representation of it) so that the\n        migration recreates the field with the same dynamic behaviour.\n        \"\"\"\n        name, path, args, kwargs = super().deconstruct()\n        if self._path_callable:\n            # Store as a string representation that can be eval\u2019d.\n            # \u201csettings.LOCAL_FILE_DIR\u201d is an example; adjust to your real setting.\n            # You could also store a reference e.g. kwargs['path'] = self._path_callable\n            # but Django only supports strings for deconstruction.\n            # Using a partial keeps the full context if needed.\n            kwargs['path'] = self._path_callable\n        return name, path, args, kwargs\n```\n\n> **Why the `deconstruct` trick works:**  \n> Django\u2019s migration writers call `deconstruct()` on each field to produce code that re\u2011creates the field. If the `path` argument is already a string (e.g. `\"\\/home/user/server_files/example_dir\"`), that literal will be written into the migration. By overriding `deconstruct()` so that it returns the *callable* instead, the migration will render something like `path=lambda: os.path.join(settings.LOCAL_FILE_DIR, 'example_dir')`, which will be evaluated on *every* migration run.\n\n---\n\n### 2.  Use the custom field in your model\n\n```python\n# myapp/models.py\nclass LocalFiles(models.Model):\n    name = models.CharField(max_length=255)\n\n    # Pass a callable that builds the directory at runtime.\n    file = DynamicFilePathField(\n        path=partial(os.path.join, settings.LOCAL_FILE_DIR, 'example_dir'),\n        recursive=True,  # optional \u2013 depends on your needs\n        match='*.*'     # optional \u2013 optional file patterns\n    )\n```\n\n> **Tip:** If you don\u2019t want to import `functools.partial`, you can just write\n> ```python\n> path=lambda: os.path.join(settings.LOCAL_FILE_DIR, 'example_dir')\n> ```\n> The lambda will be captured by `deconstruct()`, as shown above.\n\n---\n\n### 3.  Handling migrations\n\n* **Create the migration**: Run `python manage.py makemigrations`.  \n  The migration file will contain the lambda call for `path`, e.g.:\n\n  ```python\n  file = myapp.models.DynamicFilePathField(\n      path=lambda: os.path.join(settings.LOCAL_FILE_DIR, 'example_dir'),\n      ...\n  )\n  ```\n\n* **Running on prod**: When the migration is applied on a different machine, the lambda will be executed *at that point*, using the current value of `settings.LOCAL_FILE_DIR`. No hard\u2011coded path will be baked in.\n\n* **If you ever need to change the directory**: Changing the setting (i.e. `LOCAL_FILE_DIR`) only requires a fresh migration that updates the `max_length` etc.; the existing data will still point to the *old* directory because the file paths were already stored. To convert old paths to the new directory you would write a **data migration** that iterates over `LocalFiles` objects, regenerates the relative path, or sets a new column.\n\n---\n\n### 4.  Alternative approaches\n\nIf subclassing feels heavy, you can avoid `FilePathField` altogether:\n\n1. **Store the path as a normal `CharField`** and validate it in `clean()` or in a custom form field.\n2. **Use a nullable field** and fill it in a `post_save` signal by calling the callable.\n3. **Make the directory a setting** and let the front\u2011end (or users) pick files from that directory manually.\n\nSubclassing `FilePathField` is the cleanest way to retain Django\u2019s built\u2011in validation while still supporting a dynamic path.\n\n---\n\n### 5.  Summary\n\n- `FilePathField.path` cannot natively take a callable.\n- Subclass the field, evaluate the callable in `__init__`, and override `deconstruct()` to preserve the callable in migrations.\n- Use the custom field with a lambda (or `partial`) that builds the path from a setting.\n- Migrations will now call the callable at every run, preventing hard\u2011coded paths.\n\nFeel free to adapt the code snippets above to match your exact setting names and requirements.",
    "self_score": 3,
    "other_score": 1
  },
  {
    "sample_id": "django__django-11001",
    "pr": "**Answer**  \n\n---\n\n### 1. Bug overview  \n\nWhen a `QuerySet` is ordered with a `RawSQL` expression that spans several lines, Django\u2019s `SQLCompiler` incorrectly removes duplicate `ORDER BY` clauses.  \nThe culprit is the duplicate\u2011avoidance logic in `SQLCompiler.get_order_by()`, which uses the regex `self.ordering_parts` to extract the ordering key.  \nBecause the regex matches only up to the first space before the optional \u201cASC/DESC\u201d token, it captures the *last line* of the multiline SQL.  \nAll three `RawSQL` snippets in the example end with the same text (`else null end`), so only the first clause is kept and the others are dropped.\n\n---\n\n### 2. Root\u2011cause analysis  \n\n* `ordering_parts` is defined as `re.compile(r'\\s*([^\\s]+)\\s*(ASC|DESC)?\\s*$')` (simplified) \u2013 it takes the text after the last space.  \n* For multiline RawSQL the `sql` argument still contains newline characters.  \n* `self.ordering_parts.search(sql)` therefore returns a match that only sees the trailing line, e.g. `else null end`.  \n* That captured text is stored in the `seen` set.  \n* A second `RawSQL` with the same final line triggers the duplicate check and is silently omitted.\n\n---\n\n### 3. Proposed fix  \n\n1. **Pre\u2011normalize the SQL string** before feeding it into the regex.  \n2. **Only perform duplicate checks on the *canonical* representation** (whitespace\u2011normalized, no newlines).  \n3. Preserve the original clause for rendering the final SQL.\n\n#### Key changes \u2013 `django/db/models/sql/compiler.py`\n\n```diff\n-    def get_order_by(self):\n+    def get_order_by(self):\n         # existing code...\n         seen = set()\n         for ordering in self.ordering:\n-            sql = forcibly_unicode(ordering)[0]\n-            match = self.ordering_parts.search(sql)\n-            if not match:\n-                continue\n-            clause = match.group(1).strip()\n+            # Strip newlines/Unicode for duplicate detection\n+            sql_for_dup_check = ' '.join(ordering[0].splitlines()).strip()\n+            match = self.ordering_parts.search(sql_for_dup_check)\n+            if not match:\n+                continue\n+            clause = match.group(1).strip()\n             if clause in seen:\n                 continue\n             seen.add(clause)\n             # Render actual clause (keeps original formatting)\n             encoding, sql = ordering[0]\n             sql = self.context[encoding](sql)\n```\n\n* The split\u2011lines/join normalizes any internal whitespace to a single space.  \n* The duplicate check now sees the whole normalized clause, not just the last line.  \n* The original formatting is still used in the rendered SQL, so query readability is unchanged.\n\n---\n\n### 4. Tests to add\n\n```python\nimport django\nfrom django.db import models\nfrom django.db.models.expressions import RawSQL\nfrom django.test import TestCase\n\nclass RawSQLOrderingTest(TestCase):\n    def test_multiline_rawsql_order_by(self):\n        qs = MyModel.objects.all().order_by(\n            RawSQL('''\n                case when status in ('accepted', 'verification')\n                     then 2 else 1 end''', []).desc(),\n            RawSQL('''\n                case when status in ('accepted', 'verification')\n                     then (accepted_datetime, preferred_datetime)\n                     else null end''', []).asc(),\n            RawSQL('''\n                case when status not in ('accepted', 'verification')\n                     then (accepted_datetime, preferred_datetime, created_at)\n                     else null end''', []).desc())\n        raw_sql = str(qs.query)\n        # Verify all three clauses appear exactly once\n        self.assertIn(\"case when status in ('accepted', 'verification')\", raw_sql)\n        self.assertIn(\"then 2 else 1 end\", raw_sql)\n        self.assertIn(\"then (accepted_datetime, preferred_datetime)\", raw_sql)\n        self.assertIn(\"else null end)\", raw_sql)\n        # Count occurrences of \"else null end\" should be 3\n        self.assertEqual(raw_sql.count(\"else null end\"), 3)\n```\n\n* The test confirms that duplicate removal logic no longer discards subsequent clauses that share identical trailing lines.  \n* It also guards against future regressions where multiline RawSQL is used.\n\n---\n\n### 5. Impact & Backwards\u2011Compatibility  \n\n* No API changes.  \n* Existing queries using single\u2011line RawSQL are unaffected because the normalization step is idempotent.  \n* Performance impact is negligible\u2014only a small string split/join per clause.  \n* The change improves correctness for developers using multiline `RawSQL` ordering, a fairly common pattern in complex ordering logic.\n\n---\n\n**Result** \u2013 The bug is fixed: `order_by` now correctly renders all provided `RawSQL` clauses, even when they span multiple lines, and duplicate detection is based on the full clause rather than just the last line.",
    "self_score": 1,
    "other_score": 0
  },
  {
    "sample_id": "django__django-11019",
    "pr": "ANSWER: The warning is triggered by how Django builds a widget\u2019s media list by *adding* the media of each widget in the order the form fields are processed. In your form definition the fields are processed **color\u2011picker \u2192 intro \u2192 body**.  \n\n1. **ColorPicker** provides `['color\u2011picker.js']`.  \n2. **SimpleTextWidget** provides `['text\u2011editor.js']`.  \n3. **FancyTextWidget** provides `['text\u2011editor.js', 'text\u2011editor\u2011extras.js', 'color\u2011picker.js']`.\n\nWhen the form\u2019s `Media` is constructed, Django first does\n\n```python\nColorPicker().media + SimpleTextWidget().media\n```\n\nwhich yields a `Media` object whose internal order is\n\n```\n['color\u2011picker.js', 'text\u2011editor.js']\n```\n\nThe *addition* operation records that `\"color\u2011picker.js\"` must come **before** `\"text\u2011editor.js\"`.  \nIt then adds the media from `FancyTextWidget`:\n\n```python\n(previous_media) + FancyTextWidget().media\n```\n\nThe `FancyTextWidget` wants `\"text\u2011editor.js\"` first, `\"text\u2011editor\u2011extras.js\"` second and `\"color\u2011picker.js\"` last. Now the algorithm sees the same two files (`\"color\u2011picker.js\"` and `\"text\u2011editor.js\"`) in the *opposite* order to the constraint it already recorded. Hence the warning:\n\n```\nMediaOrderConflictWarning: Detected duplicate Media files in an opposite order:\ntext-editor\u2011extras.js\ntext-editor.js\n```\n\nThe warning lists `\"text\u2011editor\u2011extras.js\"` and `\"text\u2011editor.js\"` because those are the *first* pair that the comparison routine encounters when scanning the two lists, even though the actual conflicting pair is `\"text\u2011picker.js\"` vs. `\"text\u2011editor.js\"`.\n\n### What the final order **should** be\n\nGiven the dependencies you described, the correct load order is:\n\n```\n['text-editor.js', 'text-editor-extras.js', 'color-picker.js']\n```\n\nThis satisfies both the dependency of `text-editor-extras.js` on `text-editor.js` and the independent use of `color-picker.js`.\n\n### How to fix it\n\nThere are two simple ways to get rid of the warning:\n\n1. **Explicitly specify the correct order in the widget that pulls everything together** (here, `FancyTextWidget`):\n\n   ```python\n   class FancyTextWidget(forms.Widget):\n       class Media:\n           js = ('text-editor.js', 'text-editor-extras.js', 'color-picker.js')\n   ```\n\n   By listing the files in the order you actually want, Django\u2019s media merge will no longer try to infer an ordering from the earlier widgets.\n\n2. **Patch Django\u2019s `Media.__add__` (if you want a global workaround)**.  \n   The problem is that `__add__` records the *first* order it sees and later conflicts are treated as warnings. The patch would ignore conflicts when the later widget explicitly re\u2011orders the same file list. A minimal diff could look like this:\n\n   ```diff\n   # django/forms/widgets.py\n   -        for new_file in other._js:\n   +        for new_file in other._js:\n   +            # If the file is already present *and* the order matches the\n   +            # existing ordering, skip conflict detection.\n   +            if new_file in self._js and \\\n   +               self._js.index(new_file) < other._js.index(new_file):\n   +                continue\n   ```\n\n   This change relaxes the warning when the later media explicitly declares the same order.\n\n### Summary\n\n* The warning comes from Django\u2019s media\u2011merging logic, not from your widget order.  \n* The conflicting pair is `color-picker.js` vs. `text-editor.js`; the warning mistakenly highlights the first pair it encounters.  \n* Declare the correct order in `FancyTextWidget.Media` and the warning disappears. If you need a more global fix, consider patching `Media.__add__` to ignore redundant conflict checks.",
    "self_score": 1,
    "other_score": 1
  }
]