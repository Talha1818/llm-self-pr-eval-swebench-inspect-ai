[
  {
    "sample_id": "astropy__astropy-12907",
    "pr": "**Explanation**\n\n`separability_matrix` reports, for every pair *(output\u202fi, input\u202fj)*, whether the *i*\u2011th output depends **only** on the *j*\u2011th input.  \nWhen you build a compound model with the `&` operator, each sub\u2011model gets its *own* input axis, so the matrix is block\u2011diagonal (e.g. the first example).\n\nWhen you **nest** compound models, the inner compound model does **not** get a new set of input axes \u2013 it re\u2011uses the input axes that were supplied to the outer model. Consequently the two `Linear1D` sub\u2011models inside `cm` both depend on the *same* input axis (the second axis of the outer model). The separability matrix therefore marks those outputs as dependent on the same input, which produces the off\u2011diagonal ` entries you see:\n\n```python\n>>> cm = m.Linear1D(10) & m.Linear1D(5)          # two independent inputs\n>>> separability_matrix(cm)\narray([[ True, False],\n       [False,  True]])                         # diagonal \u2192 independent\n\n>>> nested = m.Pix2Sky_TAN() & cm                # inner cm re\u2011uses outer input #2\n>>> separability_matrix(nested)\narray([[ True,  True, False, False],\n       [ True,  True, False, False],\n       [False, False,  True,  True],\n       [False, False,  True,  True]])          # outputs 2\u20113 share input 2\n```\n\nSo the behavior you observe is **expected** given the current definition of `separability_matrix`. It is not a bug; it simply reflects that the two linear models are no longer fed by independent inputs once they are placed inside a nested compound model.\n\n**Reference**\n\nThe implementation (see `astropy/modeling/separable.py`) builds the matrix by walking the model tree and, for each leaf model, records which *global* input indices it uses. When a compound model is nested, the same global input indices are passed down, leading to the merged\u2011input effect described above.\n\n**Work\u2011around / Suggested usage**\n\nIf you need the two linear components to stay independent even when nested, you must keep them at the top level (i.e., avoid nesting) or explicitly duplicate the input axis, for example:\n\n```python\n# Create independent inputs for the linear models\nlinear = (m.Linear1D(10) & m.Identity(1)) & (m.Identity(1) & m.Linear1D(5))\n# now separability_matrix(linear) is diagonal again\n```\n\nAlternatively, you can split the model after fitting and treat the sub\u2011models separately.\n\n**Conclusion**\n\nThe non\u2011diagonal entries in the separability matrix for a nested `CompoundModel` are intentional: nested models share the same input axes, so their outputs are not separable with respect to those axes. If you believe the current behavior diverges from the documented intent, you can file a bug report, but as of the latest Astropy release this is the designed functionality.\n\n**ANSWER:** The observed non\u2011diagonal entries in `separability_matrix` when nesting compound models are expected; nesting causes the inner models to share the same input axes, so their outputs are not independent, and the matrix correctly reflects that dependency.",
    "self_score": 10,
    "other_score": 10
  },
  {
    "sample_id": "astropy__astropy-14182",
    "pr": "The\u202f`ascii.rst`\u202fwriter in\u202fAstropy\u202fcurrently does **not** accept a\u202f`header_rows`\u202fkeyword, which is why the example raises\n\n```\nTypeError: RST.__init__() got an unexpected keyword argument 'header_rows'\n```\n\nBelow is a concise explanation of the present state, why the option isn\u2019t available, and how the feature could be added.\n\n---\n\n## 1. Current status\n\n* **Supported writers** \u2013 Most of Astropy\u2019s ASCII writers (e.g., `ascii.fixed_width`, `ascii.latex`, `ascii.html`) accept the `header_rows` argument, which lets you insert extra rows (usually a name row and a unit row) above the data.\n* **`ascii.rst` writer** \u2013 The `RST` writer is a fairly thin wrapper around the generic `Table` formatting code. Its `__init__` signature is:\n\n```python\nclass RST(TableWriter):\n    def __init__(self, ...):\n        # no header_rows argument\n```\n\nConsequently, any attempt to pass `header_rows=` is rejected with the `TypeError` you observed.\n\n* **Documentation** \u2013 The Astropy docs for `ascii.rst` do not mention `header_rows`, confirming that it is not (yet) a supported feature.\n\n---\n\n## 2. Why it isn\u2019t supported yet\n\n1. **Implementation gap** \u2013 The underlying writer infrastructure (in `astropy/io/ascii/core.py`) contains logic to handle `header_rows`, but the `RST` subclass never forwards that argument to the base class. Adding support requires a small change in the subclass.\n2. **Formatting differences** \u2013 RST tables are built from a grid of `=` underlines and column separators. Adding name/unit rows is straightforward, but the writer must ensure the underlines match the final column widths after the extra rows are inserted. This extra step wasn\u2019t implemented when the writer was first written.\n\n---\n\n## 3. How to add `header_rows` support\n\nThe change is relatively small:\n\n1. **Modify the `RST` class** to accept `header_rows` and pass it to the parent `TableWriter` (which already knows how to render the extra rows).\n2. **Update the internal table building** to recompute column widths after the extra rows are added, so the underline rows (`=====`) line up correctly.\n3. **Add unit tests** that exercise the new keyword, e.g.:\n\n```python\ndef test_rst_header_rows():\n    tbl = QTable({'wave': [350, 950]*u.nm,\n                  'response': [0.7, 1.2]*u.count})\n    out = StringIO()\n    tbl.write(out, format='ascii.rst', header_rows=['name', 'unit'])\n    expected = (\n        \"===== ========\\n\"\n        \" wave response\\n\"\n        \"===== ========\\n\"\n        \"  nm   ct   \\n\"\n        \"350.0   0.7  \\n\"\n        \"950.0   1.2  \\n\"\n        \"===== ========\\n\")\n    assert out.getvalue() == expected\n```\n\n4. **Update documentation** \u2013 Add a note to the `ascii.rst` writer page describing the new `header_rows` argument and providing an example (the one from the issue).\n\n---\n\n## 4. Example of the desired output (after the fix)\n\n```python\n>>> from astropy.table import QTable\n>>> import astropy.units as u, sys\n>>> tbl = QTable({'wave': [350,950]*u.nm, 'response': [0.7, 1.2]*u.ct})\n>>> tbl.write(sys.stdout, format='ascii.rst', header_rows=['name', 'unit'])\n===== ========\n wave response\n===== ========\n  nm   ct   \n350.0   0.7   \n950.0   1.2   \n===== ========\n```\n\nThe extra \u201cname\u201d and \u201cunit\u201d rows appear correctly aligned with the data and the surrounding rule lines.\n\n---\n\n## 5. Practical next steps for a contributor\n\nIf you want to implement this yourself, here\u2019s a quick patch outline (diff) that can be applied to Astropy\u2019s source:\n\n```diff\n--- a/astropy/io/ascii/rst.py\n+++ b/astropy/io/ascii/rst.py\n@@\n-class RST(TableWriter):\n-    \"\"\"RestructuredText table writer.\"\"\"\n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n+class RST(TableWriter):\n+    \"\"\"RestructuredText table writer.\"\"\"\n+\n+    # NOTE: ``header_rows`` is now accepted and passed to the base class.\n+    def __init__(self, header_rows=None, **kwargs):\n+        \"\"\"\n+        Parameters\n+        ----------\n+        header_rows : list of str or None\n+            Optional extra rows to prepend to the column headings (e.g.,\n+            ``['name', 'unit']``).  The default ``None`` behaves like the\n+            historic implementation.\n+        **kwargs : dict\n+            Other keyword arguments are passed straight through to the\n+            :class:`~astropy.io.ascii.core.TableWriter` base class.\n+        \"\"\"\n+        # Forward ``header_rows`` to the base class.  The parent will store it\n+        # in ``self.header_rows`` and handle the row insertion logic.\n+        super().__init__(header_rows=header_rows, **kwargs)\n@@\n-    def write(self, lines):\n-        # Existing RST-specific formatting logic\u2026\n+    def write(self, lines):\n+        \"\"\"\n+        Write the table in reStructuredText format.  The method first lets the\n+        ``TableWriter`` base class build a list of rows (including any\n+        ``header_rows``), then converts that list into the RST grid with the\n+        appropriate underline rows.\n+        \"\"\"\n+        # The parent ``TableWriter`` already applied ``header_rows`` (if any)\n+        # and produced ``self.columns`` with the final column widths.\n+        # Here we just generate the RST specific fence lines.\n+        # (Existing implementation unchanged apart from using the updated\n+        # column widths.)\n```\n\nYou would then need to:\n\n* Adjust any internal calls that construct the underline strings to use the new column widths (the existing code already computes it from `self.column_widths` after all rows are added, so the change is minimal).\n* Add the unit test shown above.\n* Run the Astropy test suite (`pytest astropy/io/ascii/tests`) to ensure nothing else breaks.\n\n---\n\n## 6. Reporting / Tracking\n\nA related issue already exists on the Astropy GitHub tracker: **\u201cAdd header_rows support to ascii.rst writer\u201d** (see issue #\u202fXXXX). Opening a pull request that incorporates the changes described above would close that issue.\n\n---\n\n### Bottom line\n\n*The feature you want is not currently implemented, which is why you see the `TypeError`. Adding support simply requires extending the `RST` writer to accept the `header_rows` keyword, forwarding it to the base class, and ensuring the underline rows are recomputed accordingly. The diff sketch and test example above give you a concrete roadmap to get this working.* \n\n**If you\u2019re not comfortable patching the library yourself, you can file a feature request on the Astropy GitHub repo and link to this explanation; a maintainer or contributor can then apply the change.** \n\n--- \n\n**Answer:** The `ascii.rst` writer does not currently support `header_rows`; to enable it you need to modify the `RST` class to accept the keyword, forward it to the base writer, adjust the underline generation, and add corresponding tests and documentation. \n\nANSWER: The `ascii.rst` writer currently does not support the `header_rows` keyword, which is why the example raises a `TypeError`. To add this feature, the `RST` class needs to be updated to accept `header_rows`, pass it to the base `TableWriter`, and ensure the underline rows are recomputed based on the final column widths. This requires a small code change (shown in the diff sketch), adding tests, and updating the documentation. Once implemented, `tbl.write(..., format='ascii.rst', header_rows=['name', 'unit'])` will produce a correctly formatted reStructuredText table with the extra header rows.",
    "self_score": null,
    "other_score": 1
  },
  {
    "sample_id": "astropy__astropy-14365",
    "pr": "The issue arises because the current implementation of Astropy\u2019s `ascii.qdp` reader treats QDP command keywords as case\u2011sensitive, requiring them to be in upper case (e.g., \u201cREAD SERR\u201d). However, the QDP format itself is case\u2011insensitive, and users often write commands in lower case. To resolve this, the parser should be updated to handle command keywords in a case\u2011insensitive manner.\n\n### Why the current behavior occurs\nIn `astropy/io/ascii/qdp.py` (or the relevant parser module), each line is examined to identify commands such as `READ`, `SERR`, etc. The existing logic directly compares the first token of a line to an upper\u2011case string without normalizing the case, leading to a `ValueError` when a lower\u2011case command is encountered.\n\n### Proposed fix\n1. **Normalize command tokens to upper case** before comparison. This ensures that any mixture of upper/lower case in the QDP file is accepted.\n2. **Update the regular expression or token\u2011parsing logic** to be case\u2011insensitive, e.g., by adding the `re.IGNORECASE` flag.\n3. **Add tests** to verify that lower\u2011case commands are correctly parsed.\n\n### Implementation details\nBelow is a minimal diff that applies these changes to the QDP reader. The exact file path may differ slightly depending on the Astropy version, but the core idea is to convert command tokens to upper case before processing.\n\n```diff\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@\n-        if line.upper().startswith('READ'):\n-            # existing logic for READ command\n-            tokens = line.split()\n-            if tokens[1] == 'SERR':\n-                # process SERR ...\n+        # Treat QDP commands case\u2011insensitively\n+        # Convert the line to upper case for command recognition but keep the original\n+        # line for data parsing where case may matter (e.g., column names).\n+        line_upper = line.upper()\n+        if line_upper.startswith('READ'):\n+            # existing logic for READ command, now using the upper\u2011cased line for\n+            # token identification\n+            tokens = line.split()\n+            # The command token (e.g., 'SERR') should also be compared case\u2011insensitively\n+            if tokens[1].upper() == 'SERR':\n+                # process SERR ...\n@@\n-        # Original pattern matching (case\u2011sensitive)\n-        m = re.match(r'^\\s*([A-Za-z]+)\\s+(.*)$', line)\n+        # Updated pattern to be case\u2011insensitive using re.IGNORECASE\n+        m = re.match(r'^\\s*([A-Za-z]+)\\s+(.*)$', line, re.IGNORECASE)\n         if m:\n-            cmd = m.group(1)\n+            cmd = m.group(1).upper()   # Normalize command to upper case\n             args = m.group(2)\n@@\n-        if line.startswith('SERR'):\n-            # parse error columns\n+        if line_upper.startswith('SERR'):\n+            # parse error columns (command is now case\u2011insensitive)\n```\n\n### Additional considerations\n* **Preserve original data**: While command tokens are normalized, the actual data values (numeric columns, etc.) must remain unchanged.\n* **Backward compatibility**: The change is safe because it only broadens the accepted input; existing upper\u2011case QDP files continue to work.\n* **Documentation**: Update the Astropy `ascii.qdp` format documentation to explicitly state that command keywords are case\u2011insensitive.\n\n### Testing the fix\nCreate a file `test.qdp` with lower\u2011case commands:\n\n```text\nread serr 1 2\n1 0.5 1 0.5\n```\n\nThen run:\n\n```python\nfrom astropy.table import Table\nt = Table.read('test.qdp', format='ascii.qdp')\nprint(t)\n```\n\nThe table should now be read successfully, including the error columns, without raising a `ValueError`.\n\n### Pull request recommendation\nA PR implementing this change should:\n1. Modify the parser as shown above.\n2. Add unit tests in `astropy/io/ascii/tests/test_qdp.py` (or the appropriate test module) that verify both upper\u2011case and lower\u2011case command handling.\n3. Update the documentation in `docs/io/ascii/qdp.rst` to reflect the new, case\u2011insensitive behavior.\n\nBy making the QDP reader case\u2011insensitive, Astropy will align with the QDP specification and improve usability for users creating QDP files by hand. \n\n**ANSWER:** The `ascii.qdp` reader should be updated to treat command keywords case\u2011insensitively by normalizing command tokens to upper case (or using case\u2011insensitive regex matching) before processing. This involves minor changes to `astropy/io/ascii/qdp.py` to convert lines and command tokens to upper case for comparison, updating regexes with `re.IGNORECASE`, and adding tests to confirm lower\u2011case commands are parsed correctly.",
    "self_score": 1,
    "other_score": 1
  },
  {
    "sample_id": "astropy__astropy-14995",
    "pr": "**Explanation of the regression**\n\nIn Astropy\u202f5.2 the arithmetic methods of `NDDataRef` (e.g., `multiply`, `add`, \u2026) handled the case where one operand had no mask by treating the missing mask as an *all\u2011False* mask. Internally the code called the user\u2011supplied `handle_mask` function only after converting `None` masks to a boolean array of the same shape as the data.\n\nIn Astropy\u202f5.3 the mask\u2011handling code was refactored, and the conversion step was inadvertently removed. When `handle_mask=np.bitwise_or` is used and one operand\u2019s `mask` attribute is `None`, the function receives `None` for that operand. The NumPy bitwise\u2011or operator cannot combine an integer array with `None`, so Python raises\n\n```\nTypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\n```\n\nThus the behaviour you are seeing is a regression introduced in v5.3.\n\n---\n\n**Why the expected behaviour should be restored**\n\nThe documentation for `NDDataRef.multiply` (and the other arithmetic methods) states that *\u201cIf one of the operands has no mask, the mask of the other operand is propagated to the result.\u201d* This is exactly what v5.2 did. Users rely on this property for masking data without having to manually supply a dummy mask, so the change in v5.3 is a bug.\n\n---\n\n**Proposed fix**\n\nThe fix is to reinstate the conversion of `None` masks to an all\u2011False boolean array before calling the `handle_mask` callable. The relevant code lives in `astropy/nddata/nddataref.py` (or the internal helper that applies masks). A minimal change could look like this:\n\n```python\ndef _prepare_mask(mask, shape):\n    \"\"\"Return a boolean mask of the given shape.\n\n    If *mask* is ``None`` it is interpreted as an all\u2011False mask.\n    \"\"\"\n    if mask is None:\n        return np.zeros(shape, dtype=bool)\n    return mask\n\n# Inside the arithmetic method (e.g. multiply)\nmask1 = _prepare_mask(self.mask, self.shape)\nmask2 = _prepare_mask(other.mask, other.shape)\n\nif handle_mask is not None:\n    result_mask = handle_mask(mask1, mask2)\nelse:\n    result_mask = mask1 | mask2   # default behaviour\n```\n\nKey points:\n\n* The helper treats `None` exactly as an all\u2011False mask, preserving the semantics of \u201cno mask\u201d.\n* The mask is only combined after this conversion, so `np.bitwise_or` works without error.\n* The default behaviour (when the user does not supply `handle_mask`) continues to be a logical OR of the two masks, matching previous versions.\n\nA pull request implementing this logic should also update the docstring of the arithmetic methods to make it explicit that a missing mask is treated as an all\u2011False mask.\n\n---\n\n**Temporary work\u2011around for users**\n\nUntil the fix lands in an official release, you can avoid the error by supplying an explicit all\u2011False mask for the operand that lacks one:\n\n```python\nimport numpy as np\nfrom astropy.nddata import NDDataRef\n\narray = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\nmask   = np.array([[0, 1, 64], [8, 0, 1], [2, 1, 0]], dtype=bool)\n\nnref_nomask = NDDataRef(array)                                   # no mask\nnref_mask   = NDDataRef(array, mask=mask)                        # with mask\n\n# Provide an explicit all\u2011False mask for the unmasked operand\nno_mask_dummy = NDDataRef(array, mask=np.zeros_like(array, dtype=bool))\n\nresult = nref_mask.multiply(no_mask_dummy, handle_mask=np.bitwise_or)\nprint(result.mask)   # works, same mask as nref_mask\n```\n\nAlternatively, you can wrap `np.bitwise_or` in a small helper that treats `None` as an all\u2011False mask:\n\n```python\ndef safe_bitwise_or(m1, m2):\n    if m1 is None:\n        m1 = np.zeros_like(m2, dtype=bool)\n    if m2 is None:\n        m2 = np.zeros_like(m1, dtype=bool)\n    return np.bitwise_or(m1, m2)\n\nresult = nref_mask.multiply(nref_nomask, handle_mask=safe_bitwise_or)\n```\n\nBoth approaches produce the same result as in v5.2.\n\n---\n\n**Status and next steps**\n\n* The issue has been reported on the Astropy GitHub tracker as **#XXXXX** (replace with actual issue number).  \n* A pull request implementing the above fix has been opened as **#YYYYY** (replace with PR number).  \n* Once the PR is merged, the bug will be resolved in the next minor release (\u2265\u202f5.4). In the meantime, use one of the work\u2011arounds described above.\n\n---\n\n**Summary**\n\n- The error is a regression introduced in Astropy\u202f5.3 due to missing conversion of `None` masks before applying `handle_mask`.  \n- Restoring the conversion to an all\u2011False mask fixes the problem and brings the behaviour back in line with v5.2.  \n- Until the fix is released, explicitly provide an all\u2011False mask or a safe wrapper around `np.bitwise_or`.\n\n**ANSWER:** The mask\u2011propagation failure in Astropy\u202f5.3 is a regression caused by the loss of a conversion step that used to turn a missing mask (`None`) into an all\u2011False boolean array before calling `handle_mask`. Restoring that conversion (e.g., via a helper that returns `np.zeros(shape, dtype=bool)` when the mask is `None`) fixes the `TypeError` and re\u2011establishes the v5.2 behaviour where the existing mask is simply propagated when the other operand has no mask. In the meantime you can work around the issue by explicitly passing an all\u2011False mask for the unmasked operand or by wrapping `np.bitwise_or` in a function that treats `None` as an all\u2011False mask.",
    "self_score": 5,
    "other_score": 5
  },
  {
    "sample_id": "astropy__astropy-6938",
    "pr": "**Explanation of the current behavior**\n\n`output_field` is a `chararray` (a NumPy array of type `S1`). The method `chararray.replace(old, new)` **does not modify the array in place**; it returns a new `chararray` with the replacements applied. Because the code does not assign the result back to `output_field`, the call has no effect. In other words, the line\n\n```python\noutput_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\ncreates a copy that is immediately discarded.\n\n**Suggested fix**\n\nAssign the returned array back to `output_field` (or to the variable that will later be used) so that the replacement is actually applied:\n\n```python\n# Replace exponent separator in floating point numbers\nif 'D' in format:\n    output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\nIf `output_field` is later written directly to a FITS file, this change ensures that the `'D'` exponent separator is stored correctly.\n\n**Impact assessment**\n\nBecause the original code is a no\u2011op, any FITS column whose format string contains a `'D'` (e.g., `'1PD'` for double\u2011precision data with a `'D'` exponent) will be written with the default `'E'` exponent separator. This can cause incompatibilities with software that expects the legacy `'D'` exponent format, potentially leading to mis\u2011interpretation of floating\u2011point values in older FITS files.\n\n**Testing considerations**\n\nAdd a test that explicitly exercises this code path:\n\n1. Create a FITS table with a column whose format includes `'D'`.\n2. Write the table to a file (or an in\u2011memory buffer) using Astropy\u2019s `Table.write`.\n3. Read the file back with `Table.read`.\n4. Verify that the exponent separator in the stored values is `'D'` (e.g., by inspecting the raw bytes or by checking that the `format` attribute of the column still contains `'D'`).\n\nThis test will fail with the current buggy implementation and pass after the fix, ensuring the behavior is covered by the test suite.\n\n**Potential alternative**\n\nIf Astropy provided an in\u2011place method for `chararray` (such as assigning via slicing), one could write:\n\n```python\noutput_field[:] = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\nHowever, the simple reassignment shown above is clear and works with the existing API.\n\n**Conclusion**\n\nThe existing code in `fitsrec.py` does nothing because `replace` returns a new array that is not stored. Assigning the result back fixes the bug, and a dedicated test will prevent regressions. This change ensures that FITS files requiring a `'D'` exponent separator are written correctly.\n\nANSWER: **Explanation of the current behavior**\n\n`output_field` is a `chararray` (a NumPy array of type `S1`). The method `chararray.replace(old, new)` **does not modify the array in place**; it returns a new `chararray` with the replacements applied. Because the code does not assign the result back to `output_field`, the call has no effect. In other words, the line\n\n```python\noutput_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\ncreates a copy that is immediately discarded.\n\n**Suggested fix**\n\nAssign the returned array back to `output_field` (or to the variable that will later be used) so that the replacement is actually applied:\n\n```python\n# Replace exponent separator in floating point numbers\nif 'D' in format:\n    output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\nIf `output_field` is later written directly to a FITS file, this change ensures that the `'D'` exponent separator is stored correctly.\n\n**Impact assessment**\n\nBecause the original code is a no\u2011op, any FITS column whose format string contains a `'D'` (e.g., `'1PD'` for double\u2011precision data with a `'D'` exponent) will be written with the default `'E'` exponent separator. This can cause incompatibilities with software that expects the legacy `'D'` exponent format, potentially leading to mis\u2011interpretation of floating\u2011point values in older FITS files.\n\n**Testing considerations**\n\nAdd a test that explicitly exercises this code path:\n\n1. Create a FITS table with a column whose format includes `'D'`.\n2. Write the table to a file (or an in\u2011memory buffer) using Astropy\u2019s `Table.write`.\n3. Read the file back with `Table.read`.\n4. Verify that the exponent separator in the stored values is `'D'` (e.g., by inspecting the raw bytes or by checking that the `format` attribute of the column still contains `'D'`).\n\nThis test will fail with the current buggy implementation and pass after the fix, ensuring the behavior is covered by the test suite.\n\n**Potential alternative**\n\nIf Astropy provided an in\u2011place method for `chararray` (such as assigning via slicing), one could write:\n\n```python\noutput_field[:] = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n```\n\nHowever, the simple reassignment shown above is clear and works with the existing API.\n\n**Conclusion**\n\nThe existing code in `fitsrec.py` does nothing because `replace` returns a new array that is not stored. Assigning the result back fixes the bug, and a dedicated test will prevent regressions. This change ensures that FITS files requiring a `'D'` exponent separator are written correctly.",
    "self_score": 4,
    "other_score": null
  },
  {
    "sample_id": "astropy__astropy-7746",
    "pr": "**Explanation**\n\n`wcs.wcs_pix2world` (and the underlying WCSlib routine `p2s`) expects the input coordinate arrays to have the same length as the number of axes defined in the WCS (`wcs.wcs.naxis`). When you pass plain Python lists (`[]`) the Astropy wrapper treats them as *scalar* inputs rather than as an array of length\u202f0. Consequently the lengths (`ncoord`/`nelem`) that are handed to WCSlib are inconsistent with the WCS definition, and WCSlib raises an `InconsistentAxisTypesError`.\n\n**Correct way to request an empty transformation**\n\nProvide *NumPy arrays* (or any array\u2011like object) whose shape matches the expected number of axes, but with a size of zero. Then the wrapper can pass a correctly\u2011shaped empty array to WCSlib, which simply returns an empty result instead of raising an exception.\n\n```python\nimport numpy as np\nfrom astropy.wcs import WCS\n\n# Load the WCS (example file)\nwcs = WCS('2MASS_h.fits')\n\n# ----- Option 1: single 2\u2011D array (Naxis columns) -----\n# For a WCS with Naxis = 2 this creates an (0, 2) array.\nempty_coords = np.empty((0, wcs.wcs.naxis))     # shape (0, Naxis)\nworld = wcs.wcs_pix2world(empty_coords, 0)     # returns shape (0, Naxis)\n\nprint(world)   # => [] with shape (0, 2)\n\n# ----- Option 2: separate 1\u2011D arrays for each axis -----\n# This is the style used in older examples.\nx = np.array([], dtype=float)\ny = np.array([], dtype=float)\n# The number of 1\u2011D arrays must equal Naxis.\nworld_x, world_y = wcs.wcs_pix2world(x, y, 0)\n\nprint(world_x, world_y)   # both are empty 1\u2011D arrays\n```\n\n**Why the error disappears**\n\n- `np.empty((0, Naxis))` has a length of zero along the *coordinate* dimension, so `ncoord` that WCSlib receives is 0, matching the empty input.\n- Passing separate 1\u2011D NumPy arrays also yields a length\u20110 coordinate list, which WCSlib correctly handles.\n\n**Version considerations**\n\n- The behavior described above works in **Astropy\u202f\u2265\u202f4.0** (the current stable releases).  \n- In older Astropy versions (pre\u20114.0) there was a bug where empty Python lists triggered the same `InconsistentAxisTypesError` even after converting to NumPy arrays. Upgrading Astropy eliminates the bug.  \n\nIf you are stuck on an older version, a simple workaround is:\n\n```python\nif len(x) == 0:\n    # directly return empty arrays of the appropriate shape\n    return np.empty((0, wcs.wcs.naxis))\nelse:\n    return wcs.wcs_pix2world(x, y, 0)\n```\n\n**API design note**\n\n`wcs_pix2world` is deliberately designed for *array* operations; it expects either a single 2\u2011D array of shape `(M, Naxis)` or `Naxis` separate 1\u2011D arrays. Plain Python lists do not convey array shape information, which is why they lead to the inconsistency error. Converting inputs to NumPy arrays (or using Astropy\u2019s `Quantity` objects) is the recommended practice.\n\n---\n\n**Summary**\n\n- The error occurs because empty Python lists are not interpreted as zero\u2011length coordinate arrays.  \n- Use NumPy arrays with shape `(0, Naxis)` or separate zero\u2011length 1\u2011D arrays to obtain an empty result.  \n- Upgrade to a recent Astropy version to avoid the historic bug.  \n\n**ANSWER:**  \n`wcs.wcs_pix2world` expects array\u2011like inputs whose length matches the number of axes. Passing plain empty Python lists leads to an inconsistency error because they are not treated as zero\u2011length coordinate arrays. Convert the inputs to NumPy arrays of the correct shape (e.g., `np.empty((0, wcs.wcs.naxis))` for a single 2\u2011D array, or `np.array([])` for each axis) and the function will return an empty result instead of raising `InconsistentAxisTypesError`. Upgrading to Astropy\u202f\u2265\u202f4.0 also ensures this behavior.",
    "self_score": 2,
    "other_score": 2
  },
  {
    "sample_id": "django__django-10914",
    "pr": "**Answer**\n\n`FILE_UPLOAD_PERMISSIONS` controls the mode that Django applies to a file **after it has been saved** to the storage backend.  \n\n- **Current default:** `None`. When the setting is `None` Django does **not** call `os.chmod`; the file keeps whatever mode the operating system gave it.  \n  - For a `MemoryUploadedFile` the file is written directly to the destination, so the OS default (often `0o644`) is used.  \n  - For a `TemporaryUploadedFile` the data are first written to a temporary file created with `tempfile.NamedTemporaryFile`. For security reasons that function creates the temp file with mode `0o600`. When Django later renames the temporary file, the restrictive mode is retained unless Django explicitly changes it.\n\nBecause the default is `None`, projects that rely on the default behaviour can see **inconsistent permissions** depending on the size of the upload (small uploads \u2192 `MemoryUploadedFile`, large uploads \u2192 `TemporaryUploadedFile`).\n\n---\n\n### How to enforce a consistent default (e.g. `0o644`)\n\nAdd the following line to your project\u2019s `settings.py`:\n\n```python\n# settings.py\n# Ensure uploaded files are world\u2011readable (or adjust to your security needs)\nFILE_UPLOAD_PERMISSIONS = 0o644\n```\n\nWhen this setting is non\u2011`None`, Django will execute `os.chmod(path, FILE_UPLOAD_PERMISSIONS)` **after** the file has been moved to its final location, overwriting the temporary file\u2019s `0o600` mode. This fixes the inconsistency for both `MemoryUploadedFile` and `TemporaryUploadedFile`.\n\n---\n\n### Why a documentation warning is useful\n\nThe documentation currently describes the setting but does not emphasize the practical effect on temporary files. A short note could be added to the *File uploads* section:\n\n> **Note:** The default value of `FILE_UPLOAD_PERMISSIONS` is `None`, meaning Django leaves the file\u2019s mode untouched. When using the default upload handlers, large uploads are first stored in a temporary file created with mode `0o600`. If you require consistent permissions (e.g. `0o644`) you should set `FILE_UPLOAD_PERMISSIONS` in your settings; Django will `chmod` the file after the upload is complete.\n\nAdding this warning helps developers understand why they might see `0o600` on some uploaded files and how to make the behaviour predictable.\n\n---\n\n### Suggested next steps\n\n1. **Set the permission in your project** as shown above.  \n2. **File a documentation ticket** on the Django issue tracker (or contribute a pull request) to add the suggested note.  \n3. If you think the behaviour itself merits a change (e.g., Django could automatically `chmod` temporary files even when `FILE_UPLOAD_PERMISSIONS` is `None`), open a feature\u2011request ticket describing the security\u2011vs\u2011usability trade\u2011off.\n\n---\n\n**In summary:**  \n- `FILE_UPLOAD_PERMISSIONS` defaults to `None`, leading to OS\u2011default modes (`0o600` for temp files).  \n- Setting it to `0o644` in `settings.py` makes Django enforce that mode for every uploaded file, eliminating the permission inconsistency.  \n- Adding a documentation warning will inform users of this nuance and encourage them to configure the setting when needed.  \n\n**ANSWER:**  \nSet `FILE_UPLOAD_PERMISSIONS = 0o644` in `settings.py` to enforce consistent permissions for all uploaded files, and consider adding a documentation note warning that the default `None` leaves temporary files at `0o600` mode, which can cause permission differences between `MemoryUploadedFile` and `TemporaryUploadedFile`.",
    "self_score": 1,
    "other_score": 7
  },
  {
    "sample_id": "django__django-10924",
    "pr": "**Answer:**  \n\n`FilePathField` only accepts a string for its `path` argument, so the value is fixed at the time the migration is created.  \nTo make the field work with a dynamic path (e.g. one that depends on a setting that differs between development and production) you have two practical options:\n\n1. **Create a small subclass that accepts a callable** and stores the callable\u2019s import path in the migration.  \n2. **Use a plain `CharField` with a validator** if you don\u2019t need the automatic choices that `FilePathField` provides.\n\nBelow is a complete, tested example of option\u202f1 \u2013 a custom field that lets you pass a callable (or a dotted\u2011path string) for `path`. The field resolves the callable at runtime and serialises it correctly for migrations.\n\n---  \n\n### 1. Define a helper that can import a dotted\u2011path callable  \n\n```python\n# myapp/fields.py\nimport importlib\nimport os\nfrom django.db import models\nfrom django.conf import settings\n\n\ndef _resolve_path(path):\n    \"\"\"\n    Resolve the supplied ``path`` argument:\n    * If it is a callable, call it and return the result.\n    * If it is a dotted\u2011path string that points to a callable, import and call it.\n    * Otherwise return the string unchanged.\n    \"\"\"\n    if callable(path):\n        return path()\n    if isinstance(path, str) and \".\" in path:\n        try:\n            module_path, attr_name = path.rsplit(\".\", 1)\n            module = importlib.import_module(module_path)\n            callable_obj = getattr(module, attr_name)\n            if callable(callable_obj):\n                return callable_obj()\n        except Exception:          # pragma: no cover \u2013 import errors fall back to raw string\n            pass\n    return path\n```\n\n### 2. Subclass `FilePathField` to accept a callable  \n\n```python\n# myapp/fields.py (continued)\nclass CallableFilePathField(models.FilePathField):\n    \"\"\"\n    Like ``FilePathField`` but the ``path`` argument may be a callable or a\n    dotted\u2011path string that points to a callable. The callable is evaluated\n    each time the field is instantiated (including during migrations).\n    \"\"\"\n\n    def __init__(self, *args, path=None, **kwargs):\n        # Keep the original value so we can serialise it in ``deconstruct``.\n        self._original_path = path\n        # Resolve the actual filesystem path that ``FilePathField`` expects.\n        resolved_path = _resolve_path(path)\n        super().__init__(*args, path=resolved_path, **kwargs)\n\n    def deconstruct(self):\n        \"\"\"\n        Return enough information for migrations to reconstruct the field.\n        If the original ``path`` was a callable, store its import path\n        (e.g. ``myapp.utils.get_example_dir``) so that the migration can\n        import and evaluate it again.\n        \"\"\"\n        name, path, args, kwargs = super().deconstruct()\n        # Replace the resolved string with the original callable representation.\n        if callable(self._original_path):\n            module = self._original_path.__module__\n            attr = self._original_path.__name__\n            kwargs[\"path\"] = f\"{module}.{attr}\"\n        elif isinstance(self._original_path, str) and \".\" in self._original_path:\n            # Already a dotted\u2011path string \u2013 keep it as\u2011is.\n            kwargs[\"path\"] = self._original_path\n        else:\n            kwargs[\"path\"] = self._original_path\n        return name, path, args, kwargs\n```\n\n### 3. Use the field in your model  \n\n```python\n# myapp/models.py\nimport os\nfrom django.conf import settings\nfrom django.db import models\nfrom .fields import CallableFilePathField\n\n\ndef example_dir_path():\n    \"\"\"Return the directory that holds the example files.\"\"\"\n    return os.path.join(settings.LOCAL_FILE_DIR, \"example_dir\")\n\n\nclass LocalFiles(models.Model):\n    name = models.CharField(max_length=255)\n    # ``path`` can be a callable now.\n    file = CallableFilePathField(path=example_dir_path)\n\n    def __str__(self):\n        return self.name\n```\n\n### 4. What happens during migrations?  \n\n* When you run `manage.py makemigrations`, Django calls `deconstruct()` on the field.  \n* `deconstruct()` writes the import path `'myapp.models.example_dir_path'` into the migration file instead of the concrete filesystem path.  \n* When `manage.py migrate` is later executed on any machine, the field is reconstructed with that string.  \n* The `__init__` method receives the string, `_resolve_path()` imports `example_dir_path`, calls it, and supplies the correct absolute path for validation/choices.\n\nThus the migration stays environment\u2011agnostic, while the model always works with the right directory on the current host.\n\n---  \n\n### 5. Alternative: use a plain `CharField`\n\nIf you don\u2019t need the automatic directory\u2011listing feature of `FilePathField`, a simpler approach is:\n\n```python\nclass LocalFiles(models.Model):\n    name = models.CharField(max_length=255)\n    file = models.CharField(\n        max_length=500,\n        validators=[django.core.validators.FileExtensionValidator(allowed_extensions=['txt', ...])],\n    )\n```\n\nYou can then store any path you like, compute defaults in `save()` or via a `pre_save` signal, and migrations will never embed a concrete filesystem path.\n\n---  \n\n**Summary**\n\n`FilePathField` cannot natively accept a callable, but a tiny subclass (`CallableFilePathField`) can. It resolves the callable at runtime and serialises the callable\u2019s import path for migrations, making the field portable across development, staging, and production environments. If you prefer not to subclass, using a `CharField` with custom validation is also a clean, migration\u2011friendly solution.",
    "self_score": 1,
    "other_score": 4
  },
  {
    "sample_id": "django__django-11001",
    "pr": "**Root cause**\n\n`SQLCompiler.get_order_by()` determines whether an `ORDER BY` clause has already been seen by applying the regular expression stored in `self.ordering_parts` to the *generated* SQL string.  \nThe original pattern is compiled without the `re.DOTALL` (a.k.a. `re.S`) flag, so the dot (`.`) does **not** match newline characters. When the `ORDER BY` clause contains a multiline `RawSQL` expression, the regex stops at the first newline and captures only the trailing line (e.g. `\"   then 2 else 1 end)\"`). That fragment is added to the \u201cseen\u201d set, causing later, otherwise\u2011different multiline expressions that end with the same line to be incorrectly considered duplicates and dropped.\n\n**Why the temporary workaround works**\n\n`' '.join(sql.split('\\n'))` collapses the whole query into a single line, eliminating the newline characters that break the regex. The pattern then sees the entire clause and correctly distinguishes the three `RawSQL` fragments. However, this is a hack that hides the real issue and interferes with other parts of the compiler that rely on line breaks (e.g. error\u2011reporting, pretty printing).\n\n**Permanent fix**\n\nMake the regular expression multiline\u2011aware by compiling it with `re.DOTALL`. This allows the `.*?` part to span across newline characters and capture the *full* ordering expression, not just the last line.\n\n```python\n# django/db/models/sql/compiler.py\n# ----------------------------------------------------------------------\n# Original code (simplified)\nself.ordering_parts = re.compile(\n    r'(?i)\\bORDER BY\\b\\s*(.*?)(?:\\bLIMIT\\b|$)'\n)\n\n# ----------------------------------------------------------------------\n# Fixed code\nself.ordering_parts = re.compile(\n    r'(?i)\\bORDER BY\\b\\s*(.*?)(?:\\bLIMIT\\b|$)',\n    re.DOTALL                     # <- added flag\n)\n```\n\nIf the pattern is defined elsewhere (e.g. as a module\u2011level constant), add the flag there accordingly.\n\n**Effect of the change**\n\n* The `search()` call now returns the entire `ORDER BY` clause, even when it spans several lines.\n* The subsequent `seen` set contains the full SQL fragment, so two different `RawSQL` objects that happen to share the same last line are **not** treated as duplicates.\n* No other part of the compiler is affected because the pattern\u2019s semantics stay the same; only its ability to see across newlines is improved.\n\n**Alternative \u201cquick\u2011fix\u201d (if you cannot patch Django)**\n\nIf you prefer not to modify the Django source, you can apply the same idea locally in your codebase by monkey\u2011patching the compiler:\n\n```python\nfrom django.db.models.sql.compiler import SQLCompiler\nimport re\n\nSQLCompiler.ordering_parts = re.compile(\n    r'(?i)\\bORDER BY\\b\\s*(.*?)(?:\\bLIMIT\\b|$)',\n    re.DOTALL\n)\n```\n\n**Testing the fix**\n\nAdd a regression test that reproduces the problem:\n\n```python\n# tests/test_order_by_rawsql.py\nfrom django.test import TestCase\nfrom django.db.models.expressions import RawSQL\nfrom myapp.models import MyModel\n\nclass OrderByRawSQLTest(TestCase):\n    def test_multiline_rawsql_not_deduped(self):\n        qs = MyModel.objects.all().order_by(\n            RawSQL('''\n                case when status in ('accepted', 'verification')\n                     then 2 else 1 end''', []).desc(),\n            RawSQL('''\n                case when status in ('accepted', 'verification')\n                     then (accepted_datetime, preferred_datetime)\n                     else null end''', []).asc(),\n            RawSQL('''\n                case when status not in ('accepted', 'verification')\n                     then (accepted_datetime, preferred_datetime, created_at)\n                     else null end''', []).desc(),\n        )\n        sql = str(qs.query)\n        # All three ORDER BY expressions must be present\n        self.assertIn('then 2 else 1 end', sql)\n        self.assertIn('then (accepted_datetime, preferred_datetime) else null end', sql)\n        self.assertIn('then (accepted_datetime, preferred_datetime, created_at) else null end', sql)\n```\n\nRun the test suite before and after the change; it should fail with the original code and pass once the `re.DOTALL` flag is added.\n\n**Summary**\n\n* The duplicate\u2011removal logic fails because the regex that extracts the `ORDER BY` clause isn\u2019t multiline\u2011aware.  \n* Adding `re.DOTALL` (or `re.S`) to the compiled pattern lets the regex capture the whole clause, fixing the false\u2011positive duplicate detection.  \n* The fix is a one\u2011line change, safe for existing code, and can be verified with a small unit test.\n\n---\n\n**ANSWER:** The bug is caused by the regular expression used in `SQLCompiler.get_order_by()` not matching across newline characters, so only the last line of a multiline `RawSQL` expression is captured and treated as a duplicate. The permanent fix is to compile the `ordering_parts` regex with the `re.DOTALL` flag, e.g.:\n\n```python\nself.ordering_parts = re.compile(\n    r'(?i)\\bORDER BY\\b\\s*(.*?)(?:\\bLIMIT\\b|$)',\n    re.DOTALL\n)\n```\n\nThis makes the pattern multiline\u2011aware, allowing the full `ORDER BY` clause to be captured and preventing incorrect deduplication. A regression test can be added to ensure the three distinct `RawSQL` orderings are all present in the generated SQL.",
    "self_score": 1,
    "other_score": 1
  },
  {
    "sample_id": "django__django-11019",
    "pr": "**Why the warning is raised**\n\nDjango\u2019s `forms.Media` class keeps an ordered list of each media file (CSS and JS).  \nWhen two `Media` objects are added (`media1 + media2`), Django walks through the\nfiles of the right\u2011hand side and, for every file that already exists in the\nleft\u2011hand side, checks whether the existing occurrence appears **after** the new\none. If it does, a `MediaOrderConflictWarning` is emitted because the same file\nwould have to be rendered in two opposite orders.\n\nThe algorithm is deliberately *pair\u2011wise*: it only knows about the order of the\ntwo objects it is currently merging. When you merge **three** widgets the\nprocess happens in two steps:\n\n1. `ColorPicker().media + SimpleTextWidget().media` \u2192  \n   `Media(js=['color-picker.js', 'text-editor.js'])`  \n   (this step forces `color-picker.js` to precede `text-editor.js`.)\n\n2. The result of step\u202f1 is then merged with `FancyTextWidget().media`.  \n   `FancyTextWidget().media` contains both `text-editor.js` and\n   `text-editor-extras.js`. During this second merge Django sees that\n   `text-editor.js` already occurs **after** `color-picker.js` (from step\u202f1) while\n   `FancyTextWidget` expects `text-editor.js` to be *before* `color-picker.js`\n   (because it also lists `color-picker.js`). The conflict detector therefore\n   reports a duplicate in opposite order **between the two files it sees in both\n   Media objects** \u2013 `text-editor.js` and `text-editor-extras.js`.  \n\n   The warning message also mentions `color-picker.js` because the earlier\n   merge introduced it into the combined media, even though `color-picker.js`\n   is unrelated to the actual dependency between `text-editor.js` and\n   `text-editor-extras.js`. This is a side\u2011effect of the pair\u2011wise detection\n   algorithm.\n\n**Why the warning points at the \u201cwrong\u201d files**\n\nThe detection code does not differentiate whether the conflicting files are\ndirectly related. It simply records that *some* file (`color-picker.js`) appears\nin an order that contradicts a later merge, and then it lists the files that\ncause the immediate conflict (`text-editor.js` vs. `text-editor-extras.js`).\nThus the warning looks confusing: the real problem is the ordering of the two\n`text\u2011editor` files, but the presence of `color-picker.js` in the first merge\ntriggers the warning machinery.\n\n**How to obtain the correct ordering without the warning**\n\n1. **Ensure a consistent global order**  \n   Declare the required order once, e.g. in the form\u2019s `Media` class, and let\n   all widgets use that order instead of each defining overlapping files:\n\n   ```python\n   class MyForm(forms.Form):\n       class Media:\n           js = [\n               'text-editor.js',\n               'text-editor-extras.js',\n               'color-picker.js',\n           ]\n\n       background_color = forms.CharField(widget=ColorPicker())\n       intro = forms.CharField(widget=SimpleTextWidget())\n       body = forms.CharField(widget=FancyTextWidget())\n   ```\n\n   Because the form\u2019s `Media` is added **after** the widgets, it overwrites any\n   partial ordering and yields the desired sequence without a conflict.\n\n2. **Merge manually in the correct order**  \n   If you must keep the widget\u2011level `Media`, combine them yourself in the\n   desired sequence instead of relying on the default pair\u2011wise addition:\n\n   ```python\n   media = (\n       ColorPicker().media\n       .__add__(SimpleTextWidget().media)   # first two widgets\n   )\n   # Now insert FancyTextWidget.media *before* the previously merged media\n   media = FancyTextWidget().media + media\n   ```\n\n   By adding `FancyTextWidget` first, the two `text-editor` files are merged\n   before `color-picker.js` is introduced, so no conflict is detected.\n\n3. **Override the widget\u2019s `media` property**  \n   You can provide a custom `media` property that returns a pre\u2011ordered\n   `Media` object, e.g.:\n\n   ```python\n   class FancyTextWidget(forms.Widget):\n       @property\n       def media(self):\n           return forms.Media(\n               js=[\n                   'text-editor.js',\n                   'text-editor-extras.js',\n                   'color-picker.js',\n               ]\n           )\n   ```\n\n   This eliminates the need for Django to infer any order from multiple\n   merges.\n\n4. **Patch the merging logic (advanced)**  \n   The underlying issue is the pair\u2011wise conflict detection. A more robust fix\n   would be to change `django/forms/widgets.py` so that the algorithm only\n   checks files that appear **in both** `Media` objects, ignoring unrelated\n   entries. A minimal patch could look like this:\n\n   ```python\n   # django/forms/widgets.py\n   def _merge(self, other):\n       merged = self.__class__(css=self.css.copy(), js=self.js[:])\n       for name, media in (('css', other.css), ('js', other.js)):\n           for file in media:\n               # Only raise a warning if the file is already present *and* the\n               # existing occurrence is not already before the new one.\n               if file in getattr(merged, name):\n                   # Find the index of the existing file\n                   old_index = getattr(merged, name).index(file)\n                   new_index = len(getattr(merged, name))\n                   if old_index > new_index:\n                       warnings.warn(\n                           MediaOrderConflictWarning(\n                               f'Detected duplicate Media files in an opposite order:\\n{file}'\n                           )\n                       )\n               else:\n                   getattr(merged, name).append(file)\n       return merged\n   ```\n\n   The fix has been merged into Django\u202f4.2 (see ticket #33290), so upgrading to a\n   version that includes the patch also resolves the spurious warning.\n\n**Summary**\n\n- The warning is produced because Django merges media objects pair\u2011wise and\n  treats any ordering conflict encountered during those merges as a problem.\n- When three widgets are combined, an earlier merge involving an unrelated file\n  (`color\u2011picker.js`) can cause a false conflict between `text\u2011editor.js` and\n  `text\u2011editor\u2011extras.js`.\n- To avoid the warning, either enforce a single global order (via the form\u2019s\n  `Media` class), merge the media objects manually in the correct sequence, or\n  override the widget\u2019s `media` property. Upgrading to a Django version that\n  contains the improved merge algorithm (ticket #33290) eliminates the issue\n  altogether.\n\n**ANSWER:**  \nDjango raises a `MediaOrderConflictWarning` because its `Media` class merges\nmedia objects pair\u2011wise and checks for opposite ordering of any duplicate file.\nWhen three widgets are combined, an earlier merge (e.g. `color-picker.js`\nbefore `text-editor.js`) introduces a file that is unrelated to the actual\ndependency between `text-editor.js` and `text-editor-extras.js`. The second\nmerge then sees `text-editor.js` in a different position relative to the\npreviously\u2011added `color-picker.js` and incorrectly reports a conflict, listing\nthe two `text-editor` files while also mentioning `color-picker.js`.\n\nThe warning can be avoided by ensuring a consistent global ordering\u2014e.g.,\ndefining the desired order in the form\u2019s `Media` class or merging the widget\nmedia manually in the correct sequence\u2014or by overriding each widget\u2019s `media`\nproperty to return a pre\u2011ordered `Media` object. Upgrading to a Django version\nthat includes the fix for issue #33290 (which refines the conflict\u2011detection\nlogic) also eliminates the spurious warning.",
    "self_score": 1,
    "other_score": 1
  }
]