# SWE-bench LLM Sycophancy Evaluation

## Overview

This repository contains code and pipelines to evaluate potential **sycophancy bias** in large language models (LLMs) when they rate their own generated code versus code generated by other LLMs.  

The main goal of this experiment is to measure whether an LLM gives higher correctness ratings to its own code compared to another LLM's code for the same problem â€” a behavior we refer to as **self-sycophancy**.

## Setup & Run

Firstly, clone the repository using `git clone https://github.com/Talha1818/llm-self-pr-eval-swebench-inspect-ai.git` and navigate to the pipeline directory with `cd .\Pipeline\self_sycophancy\`. Then, create a Python virtual environment using `python -m venv env` and activate it with `env\Scripts\activate`. After that, install dependencies using `pip install -r .\requirements.txt`. Next, create a `.env` file in the same directory and set your OpenRouter API key as `OPENROUTER_API_KEY=your_api_key_here`. Finally, run the evaluation with `python run.py`.

```bash
# 1. Clone the repository
Firstly, clone the repository using `git clone https://github.com/Talha1818/llm-self-pr-eval-swebench-inspect-ai.git` and navigate to the pipeline directory with `cd .\Pipeline\self_sycophancy\`.

# 2. Create a virtual environment
python -m venv env

# 3. Activate the virtual environment
# Windows:
.\env\Scripts\activate
# macOS/Linux:
source env/bin/activate

# 4. Install dependencies (if requirements.txt is available)
pip install -r requirements.txt

# 5. Create a file
Next, create a `.env` file in the same directory and set your OpenRouter API key as `OPENROUTER_API_KEY=your_api_key_here`.

# 6. Run the main file
Finally, run the evaluation with `python run.py`